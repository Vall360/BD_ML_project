# -*- coding: utf-8 -*-
"""BD&ML project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ILJO9c7OGtNAFCEIsPLGhbQfEcKEIggl
"""

import pandas as pd

!pip install stocksymbol -q

from google.colab import drive
drive.mount('drive')


!pip install gnews -q


## ML
! pip install transformers -q

!pip install datasets -q
!pip install optuna -q
! pip install -U accelerate
!pip install evaluate -q
##!pip install pandas==2.2.1

from transformers import AutoModelForSequenceClassification
from transformers import TrainingArguments
import numpy as np
import evaluate
from transformers import AutoTokenizer
from datasets import Dataset


!pip install stargazer
from stargazer.stargazer import Stargazer

yf.__version__

!pip install yfinance

import yfinance as yf

"""## Parsing all cycle

### Classical parser
"""

!pip install dateparser -q

## Source https://github.com/kotartemiy/pygooglenews/blob/master/pygooglenews/__init__.py
import feedparser
from bs4 import BeautifulSoup
import urllib
from dateparser import parse as parse_date
import requests



class GoogleNews:
    def __init__(self, lang = 'en', country = 'USA'):
        self.lang = lang.lower()
        self.country = country.upper()
        self.BASE_URL = 'https://news.google.com/rss'

    def __top_news_parser(self, text):
        """Return subarticles from the main and topic feeds"""
        try:
            bs4_html = BeautifulSoup(text, "html.parser")
            # find all li tags
            lis = bs4_html.find_all('li')
            sub_articles = []
            for li in lis:
                try:
                    sub_articles.append({"url": li.a['href'],
                                         "title": li.a.text,
                                         "publisher": li.font.text})
                except:
                    pass
            return sub_articles
        except:
            return text

    def __ceid(self):
        """Compile correct country-lang parameters for Google News RSS URL"""
        return '?ceid={}:{}&hl={}&gl={}'.format(self.country,self.lang,self.lang,self.country)

    def __add_sub_articles(self, entries):
        for i, val in enumerate(entries):
            if 'summary' in entries[i].keys():
                entries[i]['sub_articles'] = self.__top_news_parser(entries[i]['summary'])
            else:
                entries[i]['sub_articles'] = None
        return entries

    def __scaping_bee_request(self, api_key, url):
        response = requests.get(
            url="https://app.scrapingbee.com/api/v1/",
            params={
                "api_key": api_key,
                "url": url,
                "render_js": "false"
            }
        )
        if response.status_code == 200:
            return response
        if response.status_code != 200:
            raise Exception("ScrapingBee status_code: "  + str(response.status_code) + " " + response.text)

    def __parse_feed(self, feed_url, proxies=None, scraping_bee = None):

        if scraping_bee and proxies:
            raise Exception("Pick either ScrapingBee or proxies. Not both!")

        if proxies:
            r = requests.get(feed_url, proxies = proxies)
        else:
            r = requests.get(feed_url)

        if scraping_bee:
            r = self.__scaping_bee_request(url = feed_url, api_key = scraping_bee)
        else:
            r = requests.get(feed_url)


        if 'https://news.google.com/rss/unsupported' in r.url:
            raise Exception('This feed is not available')

        d = feedparser.parse(r.text)

        if not scraping_bee and not proxies and len(d['entries']) == 0:
            d = feedparser.parse(feed_url)

        return dict((k, d[k]) for k in ('feed', 'entries'))

    def __search_helper(self, query):
        return urllib.parse.quote_plus(query)

    def __from_to_helper(self, validate=None):
        try:
            validate = parse_date(validate).strftime('%Y-%m-%d')
            return str(validate)
        except:
            raise Exception('Could not parse your date')



    def top_news(self, proxies=None, scraping_bee = None):
        """Return a list of all articles from the main page of Google News
        given a country and a language"""
        d = self.__parse_feed(self.BASE_URL + self.__ceid(), proxies=proxies, scraping_bee=scraping_bee)
        d['entries'] = self.__add_sub_articles(d['entries'])
        return d

    def topic_headlines(self, topic: str, proxies=None, scraping_bee=None):
        """Return a list of all articles from the topic page of Google News
        given a country and a language"""
        #topic = topic.upper()
        if topic.upper() in ['WORLD', 'NATION', 'BUSINESS', 'TECHNOLOGY', 'ENTERTAINMENT', 'SCIENCE', 'SPORTS', 'HEALTH']:
            d = self.__parse_feed(self.BASE_URL + '/headlines/section/topic/{}'.format(topic.upper()) + self.__ceid(), proxies = proxies, scraping_bee=scraping_bee)

        else:
            d = self.__parse_feed(self.BASE_URL + '/topics/{}'.format(topic) + self.__ceid(), proxies = proxies, scraping_bee=scraping_bee)

        d['entries'] = self.__add_sub_articles(d['entries'])
        if len(d['entries']) > 0:
            return d
        else:
            raise Exception('unsupported topic')

    def geo_headlines(self, geo: str, proxies=None, scraping_bee=None):
        """Return a list of all articles about a specific geolocation
        given a country and a language"""
        d = self.__parse_feed(self.BASE_URL + '/headlines/section/geo/{}'.format(geo) + self.__ceid(), proxies = proxies, scraping_bee=scraping_bee)

        d['entries'] = self.__add_sub_articles(d['entries'])
        return d

    def search(self, query: str, helper = True, when = None, from_ = None, to_ = None, proxies=None, scraping_bee=None):
        """
        Return a list of all articles given a full-text search parameter,
        a country and a language
        :param bool helper: When True helps with URL quoting
        :param str when: Sets a time range for the artiles that can be found
        """

        if when:
            query += ' when:' + when

        if from_ and not when:
            from_ = self.__from_to_helper(validate=from_)
            query += ' after:' + from_

        if to_ and not when:
            to_ = self.__from_to_helper(validate=to_)
            query += ' before:' + to_

        if helper == True:
            query = self.__search_helper(query)

        search_ceid = self.__ceid()
        search_ceid = search_ceid.replace('?', '&')

        d = self.__parse_feed(self.BASE_URL + '/search?q={}'.format(query) + search_ceid, proxies = proxies, scraping_bee=scraping_bee)

        d['entries'] = self.__add_sub_articles(d['entries'])
        return d

import datetime
gn = GoogleNews(lang = 'en')

## function por parsing several days
##Created to break limit of 100 publications per request by Googgle
##Credits to https://stackoverflow.com/questions/72571345/googlenews-date-range
def get_news(search):
   stories = []
   start_date = datetime.date(2020,1,1)
   end_date = datetime.date(2023,9,1)
   print(start_date, end_date)
   delta = datetime.timedelta(days=5)  ## timestamp object to datetime
   date_list = pd.date_range(start_date, end_date).tolist()
   ##print(date_list)

   for date in date_list[:-1]:
    x = (date).strftime('%Y-%m-%d')
    x1 = (date+delta).strftime('%Y-%m-%d')
    ##print(x1)
    result = gn.search(search, from_=(date).strftime('%Y-%m-%d'), to_=(date+delta).strftime('%Y-%m-%d'))
    newsitem = result['entries']

    for item in newsitem:
        story = {
            'title':item.title,
            'link':item.link,
            'published':item.published
        }
        stories.append(story)
   print("Finished parsing of:", search,"for dates:", start_date, '-', end_date )
   return stories

"""### Parsing"""

from stocksymbol import StockSymbol

## Getting stock tickers from S&P 500 index
api_key = '4231f700-99e1-42ec-b8a8-03b27d3695f3'
ss = StockSymbol(api_key)
symbol_list_us = ss.get_symbol_list(index="SPX")
Tickers = pd.DataFrame(symbol_list_us)

Tickers.to_csv('symbol_list_selected.csv')
!cp symbol_list_selected.csv "/content/drive/MyDrive/Colab Notebooks/Data_paper"

## Read list of stock tickers

symbol_list = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Data_paper/symbol_list_selected.csv')

symbol_list

##Given that parsing was done in steps we manually adjust already parsed stock tickers
symbol_list.loc[:40, 'Parsed'] =1
symbol_list.to_csv('symbol_list_selected.csv')
!cp symbol_list_selected.csv "/content/drive/MyDrive/Colab Notebooks/Data_paper"

## init dataset to write to

News_dataset = pd.DataFrame(list())
News_dataset.to_csv('News_dataset.csv')
##!cp News_dataset.csv "/content/drive/MyDrive/Colab Notebooks/Data_paper"

News_dataset = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Data_paper/News_dataset.csv')

News_dataset

del News_dataset

Symbol_list = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Data_paper/symbol_list_selected.csv')

##We parse only ~50 top tickers
Symbol_list =Symbol_list.head(51)

##parsing function to collect news from Google News
for index, row in Symbol_list.iterrows():
  if row['Parsed'] == 0:
      ## Run function to get publications from the short name of company
      List_of_news = get_news(row['shortName'])

      Data_frac = pd.DataFrame(List_of_news)

      ## Append additional columns
      Data_frac['exchange'] = row['exchange']
      Data_frac['symbol'] = row['symbol']
      Data_frac.to_csv('/content/drive/MyDrive/Colab Notebooks/Data_paper/News_dataset.csv', mode='a', header=False, escapechar='\\')


      ## Run function to get publications from the ticker of company
      List_of_news2 = get_news(row['symbol'])
      Data_frac = pd.DataFrame(List_of_news2)

      ## Append additional columns
      Data_frac['exchange'] = row['exchange']
      Data_frac['symbol'] = row['symbol']
      Data_frac.to_csv('/content/drive/MyDrive/Colab Notebooks/Data_paper/News_dataset.csv', mode='a', header=False, escapechar='\\')


  else:
    print(row['shortName'], 'is already parced')

"""### Data Overview"""

News_dataset = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Data_paper/News_dataset.csv', sep=",")

News_dataset.row.str.extract('(?P<Title>.*?) - ((?P<Source>[A-Z ]*$)|(?P<county>.*?), (?P<state_code>[A-Z]{2}$))')



test1 = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Data_paper/News_datasetHeader.csv', header=0, sep=",")

import plotly.express as px
to_plot = test1.groupby(test1[' Tiker'])

figa = px.bar(to_plot['Index'].count(), title='Number of publications from 01.2020 to 09.2023')
figa.update_layout(showlegend=False)
figa.update_xaxes(title='Ticker', visible=True, showticklabels=True)

import yfinance as yf

test1 = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Data_paper/News_datasetHeader.csv', header=0, sep=",")

test1

##List of companies in dataset

Companies = test1[' Tiker'].unique()
Companies = Companies.tolist()

##Get financial data
from stocksymbol import StockSymbol
Stock_data = pd.DataFrame()

for comp in Companies:
  ticker = yf.Ticker(comp)
  hist = ticker.history(period="4y")
  hist['symbol'] = comp
  Stock_data = Stock_data.append(hist)

Stock_data.columns

Stock_data.to_csv('Stock_data.csv')
!cp Stock_data.csv "/content/drive/MyDrive/Colab Notebooks/Data_paper"

Stock_data

import plotly.express as px
fig = px.line(Stock_data,color='symbol', y='Close')
fig.update_yaxes(title='Adj. Close', visible=True, showticklabels=True)

"""##Project"""

##import news DataFrame
names = ["index","Title", 'link', 'Date', 'Ex.', 'Company']
News_dataset = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Data_paper/News_dataset.csv', header=0, names = names, sep=",")

import numpy as np
import spacy
from sklearn.cluster import DBSCAN
import yfinance as yf
from tqdm import tqdm
tqdm.pandas()

from pandas.tseries.holiday import USFederalHolidayCalendar
from pandas.tseries.offsets import BDay

import yfinance as yf

import tensorflow as tf
import torch



from sklearn.preprocessing import OneHotEncoder

from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import f1_score
from sklearn.metrics import roc_auc_score

import lightgbm as lgb

import optuna
from optuna.samplers import TPESampler


random_state=42

class ML_build:

  ## init class
  def __init__(self):
    return

  ## assign news dataframe into class
  def news_df(self, df):


    self.df_news = df.copy()
    self.df_news.loc[:,'Date'] = pd.to_datetime(self.df_news.loc[:,'Date'])
    return

  ## data cleaning from duplicates, other languages
  def data_preparation(self, title_column = 'Title',
                       drop_dup=True, sources_sep = True, Non_eng = 'drop'):
    ## Drop duplicates. Requiered step Given that we parsed for each company twice
    if drop_dup ==True:
      self.df_news = self.df_news.drop_duplicates()
    else:
      print("duplicates were not excluded")

    ## Drop Non-english news
    if Non_eng == 'drop':
      def isEnglish(s):
        return s.isascii()

      self.df_news = self.df_news[self.df_news[title_column].apply(isEnglish) == True]
    else:
      print("other languages were not excluded")


    if 'Source' not in self.df_news:
      self.df_news[['Title', 'Source']] = self.df_news['Title'].str.rsplit(' - ', n=1, expand=True)
    else:
      pass
    return


  ## finding important new by clusterisation based on rare words
  def first_story_flag(self, Light_model = False):
    if Light_model == False:
      ##advanced vectorisation. Does not support large df.
      sent_vecs = {}
      docs = []
      spacy.cli.download('en_core_web_lg')
      nlp = spacy.load('en_core_web_lg')
      for title in tqdm(self.df_news.Title.head(1000)):
        doc = nlp(title)
        docs.append(doc)
        sent_vecs.update({title: doc.vector})
        sentences = list(sent_vecs.keys())
        vectors = list(sent_vecs.values())
      x = np.array(vectors)
      ## search for optimal eps
      n_classes = {}
      for i in tqdm(np.arange(0.001, 1, 0.002)):
        dbscan = DBSCAN(eps=i, min_samples=2, metric='cosine').fit(x)
        n_classes.update({i: len(pd.Series (dbscan. labels_) .value_counts ())})
      ##print(n_classes)
      res_list = []
      for item in dbscan.labels_:
          if item not in res_list:
              res_list.append(item)
      ##print(res_list)


      ## Get most sensitive classification
      optimal_eps   = max(n_classes, key=n_classes.get)



      self.df_news["DBSCAN"] = -1
      self.df_news["First_story"] = 0
      self.df_news.set_index('Title',inplace=True)


      for i in self.df_news['Company'].unique():

        docs = []
        sent_vecs = {}
        titles = []


        ##for title in tqdm(self.df_news.loc[(self.df_news['Company'] == i), 'Title']):
        for title in tqdm(self.df_news.loc[(self.df_news['Company'] == i)].index.tolist()):
          titles.append(title)
          doc = nlp(title)
          docs.append(doc)
          sent_vecs.update({title: doc.vector})
          sentences = list(sent_vecs.keys())
          vectors = list(sent_vecs.values())
        x = np.array(vectors)

        edges = zip(*nx.to_edgelist(G))
        G1 = igraph.Graph(len(G), zip(*edges[:2]))
        D = 1 - np.array(G1.similarity_jaccard(loops=False))

        dbscan = DBSCAN(D, metric='precomputed', eps=optimal_eps, min_samples=2)

        ##dbscan = DBSCAN(eps=optimal_eps, min_samples=2, metric='cosine').fit(x)

        ##print(dbscan.labels_.shape, titles.shape)

        titles_dbscan = self.df_news.loc[(self.df_news['Company'] == i)].index.tolist()
        titles_dbscan = list(sent_vecs.keys())
        '''result_dbscan = pd.DataFrame({'DBSCAN': dbscan.labels_,
                                      'sent': titles_dbscan})
        if i == self.df_news['Company'].unique()[0]:
          self.df_news = self.df_news.merge(result_dbscan,
                                          left_on='Title', right_on='sent')
        else:
          self.df_news.loc[(self.df_news['Company'] == i), ]'''
        result_dbscan = pd.DataFrame({'DBSCAN': dbscan.labels_,
                                      'Title': titles_dbscan})
        result_dbscan.set_index('Title',inplace=True)

        Story_list = []
        def first_occurance(value):
          if value in Story_list:
            return 0
          else:
            Story_list.append(value)
            return 1

        result_dbscan['First_story'] = result_dbscan['DBSCAN'].apply(first_occurance)
        self.df_news.update(result_dbscan)
      return

      ##self.df_news = self.df_news.rename(columns={"A": "a", "B": "c"})


    else:
      ## run TF-ID model Based on http://ai.intelligentonlinetools.com/ml/text-clustering-word-embedding-machine-learning/
      from sklearn.feature_extraction.text import TfidfVectorizer

      vec = TfidfVectorizer( stop_words='english')##,
                      ##ngram_range=(1, 2))
      x = vec.fit_transform(self.df_news.head(1000).Title)
      ## search for optimal eps
      n_classes = {}
      for i in tqdm(np.arange(0.001, 1, 0.002)):
        dbscan = DBSCAN(eps=i, min_samples=2, metric='cosine').fit(x)
        n_classes.update({i: len(pd.Series (dbscan. labels_) .value_counts ())})
      print(n_classes)

      ## init full dataset

      Final_DBSCAN_df = pd.DataFrame()
      for company in tqdm(self.df_news['Company'].unique()):
        df_train = self.df_news[self.df_news['Company'] == company]
        vec = TfidfVectorizer( stop_words='english')##,
                        ##ngram_range=(1, 2))
        x = vec.fit_transform(self.df_news.Title)

        ## Get most sensetive classification
        optimal_eps   = max(n_classes, key=n_classes.get)
        dbscan = DBSCAN(eps=optimal_eps, min_samples=2, metric='cosine').fit(x)

        result_dbscan = pd.DataFrame({'DBSCAN': dbscan.labels_,
                                      'sent': self.df_news.Title})

        Final_DBSCAN_df = pd.concat([Final_DBSCAN_df, result_dbscan])
      ##print(Final_DBSCAN_df)
      self.df_news = self.df_news.merge(Final_DBSCAN_df,
                                        left_on='Title', right_on='sent')


    ## Get flag for first row in class
    Story_list = []
    def first_occurance(value):
      if value in Story_list:
        return 0
      else:
        Story_list.append(value)
        return 1

    self.df_news['First_story'] = self.df_news['DBSCAN'].apply(first_occurance)

    self.df_news.reset_index(inplace=True)


    return


  ## inital semtiment analysis


    return

  def BERT_sentiment_test(self, column = 'Title' ):
    from datasets import Dataset
    from transformers.pipelines.pt_utils import KeyDataset
    from transformers import pipeline

    nlp = pipeline(task='sentiment-analysis',
               model='ProsusAI/finbert', device=-1, batch_size=1000)


    result = []

    dataset = Dataset.from_pandas(self.df_news)

    for out in tqdm(nlp(KeyDataset(dataset, 'Title'))):
      result.append(out)
    print(out)
    result = pd.DataFrame(result)
    ##result.cou

    self.df_news= self.df_news.join(result.set_axis(self.df_news.index))

    ##self.df_news['FinBERT'] = result.label
    ##self.df_news['Fin_BERT_score'] = result.score
    return

  def BERT_quality_test(self, column = 'Title' ):
    from datasets import Dataset
    from transformers.pipelines.pt_utils import KeyDataset
    from transformers import pipeline

    nlp = pipeline("text-classification", model="ikoghoemmanuell/finetuned_fake_news_roberta",
                   device=-1, batch_size=1000)


    result = []

    dataset = Dataset.from_pandas(self.df_news)

    for out in tqdm(nlp(KeyDataset(dataset, 'Title'))):
      result.append(out)
    result = pd.DataFrame(result)

    self.df_news= self.df_news.join(result.set_axis(self.df_news.index))


    ##self.df_news['FakeBERT'] = result.label
    ##self.df_news['FakeBERT_score'] = result.score
    return

  def BERT_NER_test(self, column = 'Title' ):
    from datasets import Dataset
    from transformers.pipelines.pt_utils import KeyDataset
    from transformers import pipeline

    nlp = pipeline(task='ner',
               model='dslim/bert-base-NER', device=-1, batch_size=1000)

    result = []
    dataset = Dataset.from_pandas(self.df_news)

    for out in tqdm(nlp(KeyDataset(dataset, 'Title'))):
      out1 = [i['word'] for i in out]
      out2 = [i['entity'] for i in out]

      output = {}
      output =dict.fromkeys(out2, out1)
      result.append(output)

    result = pd.DataFrame(result)

    ##self.df_news = pd.concat([self.df_news, result], axis=1, ignore_index=True)

    self.df_news= self.df_news.join(result.set_axis(self.df_news.index))

    return

  def BERT_Relevance(self, column = 'Title' ):
    from datasets import Dataset
    from transformers.pipelines.pt_utils import KeyDataset
    from transformers import pipeline

    model = AutoModelForSequenceClassification.from_pretrained("/content/drive/MyDrive/Colab Notebooks/Data_paper/Relevance_model")

    tokenizer = AutoTokenizer.from_pretrained("ProsusAI/finbert")
    nlp = pipeline("text-classification", model=model, tokenizer =tokenizer,
                  device=0, batch_size=1000)


    result = []

    ##some telegram news dont have title. pass source back
    ##self.df_news["Title"].fillna(self.df_news["Source"], inplace = True)
    self.df_news["Title"] = self.df_news["Title"] + self.df_news["Source"]

    dataset = Dataset.from_pandas(self.df_news)

    for out in tqdm(nlp(KeyDataset(dataset, 'Title'))):
      print(out)
      result.append(out)
    result = pd.DataFrame(result)

    self.df_news.drop(['label', "score"], axis=1, inplace=True)

    self.df_news= self.df_news.join(result.set_axis(self.df_news.index))


    ##self.df_news['FakeBERT'] = result.label
    ##self.df_news['FakeBERT_score'] = result.score
    return





  def save_BERT(self):
    self.df_news.to_csv('BERT_project_results.csv')
    !cp BERT_project_results.csv "/content/drive/MyDrive/Colab Notebooks/Data_paper"
    return

  def load_BERT(self):
    del self.df_news

    ##Currently we work with 100000 publications (less than 2% of dataset)
    ## It is done to ensure swift working of the functions bellow.
    self.df_news = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Data_paper/BERT_project_results.csv',
                               low_memory=False) ##, nrows=10000)

    self.df_news.set_index('Unnamed: 0',inplace=True)
    ##self.df_news.drop(['Unnamed: 0.1', 'index'], axis=1, inplace=True)




    ##self.df_news = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Data_paper/BERT_project_results.csv',
    ##                        low_memory=False)


    return

  def save_BERT_DBSCAN(self):
    self.df_news.to_csv('BERT_DBSCAN_project_results.csv')
    !cp BERT_project_results.csv "/content/drive/MyDrive/Colab Notebooks/Data_paper"
    return

  def load_BERT_DBSCAN(self):
    del self.df_news


    self.df_news = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Data_paper/BERT_DBSCAN_project_results.csv',
                               low_memory=False)
    self.df_news.set_index('Unnamed: 0',inplace=True)

    return






  ##Get Source feature, adjust time of publication
  def time_publication_adjustment(self):
    ## assume lag of google news is several hours
    self.df_news['Date'] = pd.to_datetime(self.df_news['Date'])


    ## move all publications of Saturday and Sunday to Monday
    print(self.df_news['Date']) ##+ pd.offsets.CustomBusinessDay(n=1, calendar=USFederalHolidayCalendar()))
    ##self.df_news.loc[:,'Date'] = self.df_news['Date']+ pd.offsets.BusinessDay(0)

    def offset_date(start, offset):
      return start + pd.offsets.CustomBusinessDay(n=offset, calendar=USFederalHolidayCalendar())

    offset = 0
    self.df_news['Date']= self.df_news['Date'].apply(lambda x: offset_date(x,
                                          offset))
    self.df_news['Date'] = self.df_news['Date'].dt.date
    print(self.df_news['Date'])

    self.df_news['Source'] = self.df_news['Title'].str.split(' - ').str[1]

    return



  ## request stock market data from
  def import_stock_data(self):
    Companies = self.df_news['Company'].unique()
    Companies = Companies.tolist()

    Stock_data = pd.DataFrame()

    ##Get index returns
    ticker = yf.Ticker('^GSPC')
    SP500 = ticker.history(period="5y")
    SP500["Returns"] = (SP500.Close - SP500.Open)/SP500.Open

    ##Technical transformations
    SP500 =SP500.reset_index()
    SP500.loc[:,'Date'] = pd.to_datetime(SP500.loc[:,'Date'])
    SP500['Date'] = SP500['Date'].dt.date


    ##stock data for each company
    for comp in Companies:
      try:
        ticker = yf.Ticker(comp)
        hist = ticker.history(period="5y")

        ##Technical transformations
        hist =hist.reset_index()
        hist.loc[:,'Date'] = pd.to_datetime(hist.loc[:,'Date'])
        ##print(hist['Date'])
        hist['Date'] = hist['Date'].dt.date
        hist['Date'] = pd.to_datetime(hist['Date'])

        ##print(hist)
        ##test_1 = apple.history(period="5y")
        ##test_1.set_index(pd.to_datetime(pd.to_datetime(test_1.reset_index()['Date']).dt.date), inplace=True)


        stock_count = pd.DataFrame(ticker.financials)
        ##print(stock_count)
        stock_count = stock_count.T
        stock_count = stock_count['Diluted Average Shares'].sort_index()
        stock_count = pd.DataFrame(stock_count)

        tol = pd.Timedelta('7 day')
        hist = pd.merge_asof(hist, stock_count, right_index=True,left_on='Date', direction='nearest',tolerance=tol)


        hist['Diluted Average Shares'].fillna(method='ffill', inplace=True)
        hist['Diluted Average Shares'].fillna(method='bfill', inplace=True)
        ##hist['Diluted Average Shares'].ffill(inplace=True).bfill(inplace=True)
        hist["Market_Cap"] = hist["Diluted Average Shares"] * hist["Close"]

        hist['Date'] = hist['Date'].dt.date


        try:
          Earn_releases = ticker.get_earnings_dates(limit=25)

          ##Technical transformations
          Earn_releases.reset_index(inplace=True)
          Earn_releases.rename(columns={'Earnings Date':'Date'}, inplace=True)
          Earn_releases.loc[:,'Date'] = pd.to_datetime(Earn_releases.loc[:,'Date'])
          Earn_releases['Date'] = Earn_releases['Date'].dt.date

          hist = pd.merge(hist, Earn_releases, how="left", on='Date')
        except:
          print("Error in parcing releases for", comp)

        ## append symbol
        hist['symbol'] = comp

        ##Add index returns for each company
        hist['SP500_returns'] = SP500["Returns"]
        hist['SP500_returns_yesterday'] = SP500["Returns"].shift(-1)

        ##Stock_data = Stock_data.append(hist) ##Change later
        ##print(hist)
        Stock_data = pd.concat([Stock_data, hist], ignore_index=True)
      except:
        print(comp, "not found on yfinance")

    ##Create returns variable
    Stock_data["Returns"] = (Stock_data.Close - Stock_data.Open)/Stock_data.Open
    Stock_data["Returns_binary"] = 0
    Stock_data.loc[Stock_data["Returns"]>0,["Returns_binary"]] = 1
    ## Create previos returns feature
    Stock_data["Returns_yesterday"] = Stock_data["Returns"].shift(-1)

    '''##Technical transformations
    Stock_data =Stock_data.reset_index()
    Stock_data.loc[:,'Date'] = pd.to_datetime(Stock_data.loc[:,'Date'])
    Stock_data['Date'] = Stock_data['Date'].dt.date'''

    self.df_stock = Stock_data
    return

  ##function to get features from BERT models, publishers
  def One_hot_encode(self, Interactions = True, OHE = True):
    ##self.df_news.Title.str.rsplit(" - ", n=1, expand=True)
    ##print(self.df_news.columns)
    self.df_news.reset_index(drop=True, inplace=True)

    ##We have too many Sources. We drop all except top 10.
    ##Credit to https://stackoverflow.com/questions/32511061/remove-low-frequency-values-from-pandas-dataframe

    '''threshold = 10 # Anything that occurs less than this will be removed.
    value_top10 = self.df_news.sort_values(['Source'], ascending=False).head(10)
    to_remove = self.df_news[self.df_news.Source != value_top10].index
    self.df_news.Source.replace(to_remove, 'Other', inplace=True)'''




    if OHE == True:

      ## init Encoder
      OHE = OneHotEncoder(categories='auto', sparse=True)

      ##selected columns for encoding
      columns_ohe= ['FinBERT', 'FakeBERT'] ## simple features currently.


      ## NOTE: OneHotEncoder has mamory problems on large dataset
      ## Current roundabout solution

      ##cycle to decrease memory usage.


      ##feature_arr = ohe.fit_transform(self.df_news[one_column]).toarray()
      OHE.fit(self.df_news[columns_ohe].head(1000))
      feature_arr = OHE.transform(self.df_news[columns_ohe]).toarray()
      feature_labels = OHE.categories_

      feature_labels = np.array(feature_labels, dtype=object).ravel()
      feature_labels = [item for sublist in feature_labels for item in sublist]
      ##print(feature_arr)
      ##print(feature_labels)
      features = pd.DataFrame(feature_arr, columns=feature_labels)
      print(features)

      ##self.df_news = pd.concat([self.df_news, features], axis=1)
      self.df_news = self.df_news.join(features.set_axis(self.df_news.index))

    else:

      cols1 = ['negative', 'neutral', 'positive']
      for col in tqdm(cols1):
        self.df_news[col] = 0
        self.df_news.loc[self.df_news['FinBERT'] == col, col] = 1

      cols2 = ['LABEL_0', 'LABEL_1']
      for col in tqdm(cols2):
        self.df_news[col] = 0
        self.df_news.loc[self.df_news['FakeBERT'] == col, col] = 1

      cols3 = ['Rel_neg', 'Rel_neut', 'Rel_pos'] ## new names for Relevance model
      for index in tqdm([0, 1, 2]):
        New_name = cols3[index]
        Old_name = cols1[index]
        self.df_news[New_name] = 0
        self.df_news.loc[self.df_news['label'] == Old_name, New_name] = 1
      ##self.df_news[['negative', 'neutral', 'postive']]= pd.get_dummies(
       ##   self.df_news['FinBERT'], dtype=float)
      ##cols2 = ['LABEL_0', 'LABEL_1']
      ##self.df_news[['LABEL_0', 'LABEL_1']]= pd.get_dummies(
       ##   self.df_news['FakeBERT'], dtype=float)

    ##interactions between variables
    if Interactions == True:
      ## NER BERT: dummy = 1 if there is organisation in text
      self.df_news.loc[:, 'B-ORG'] = self.df_news['B-ORG'].notnull().astype('int')
      self.df_news.drop(['I-ORG', 'B-MISC', 'B-PER', 'I-PER',
                         'B-LOC', 'I-LOC', 'I-MISC'], axis=1,
                        inplace=True)

      self.df_news['positive*Label_1'] = self.df_news['positive'] * self.df_news['LABEL_1']
      self.df_news['negative*Label_1'] = self.df_news['negative'] * self.df_news['LABEL_1']
      return
    else:
      return

  ## merging dataframes to fit in classifiers
  def prepare_final(self, Interactions = True):

    ## To solve the error when running time_publication_adjustment(naive = False)
    self.df_news['Date'] = pd.to_datetime(self.df_news['Date']).dt.date


    ##Groupby and combine datasets
    New_indicators= self.df_news.groupby(['Company', 'Date']).sum()
    self.df = pd.merge(self.df_stock, New_indicators, left_on=['symbol', 'Date'],
                       right_on=['Company', 'Date'])
    self.df.set_index('Date', inplace = True)

    ## Market uncertanty index
    for date in self.df.index.unique():
      Set = self.df[self.df.index == date]
      Positive_sum = Set['positive'].sum()
      Negative_sum = Set['negative'].sum()
      try:
        Uncert = max(0, 1- Negative_sum / (Positive_sum + Negative_sum))
      except:
        Uncert = 0
      self.df.loc[self.df.index  == date, 'Uncert']  = Uncert

    if Interactions == True:
      self.df['positive*Label_1*Uncert'] = self.df['positive*Label_1'] * self.df.Uncert
      self.df['negative*Label_1*Uncert'] = self.df['negative*Label_1'] * self.df.Uncert
    else:
      pass


    ##drop outside columns: current stock data, leftover columns
    self.df.drop(['Open', 'High', 'Low', 'Close', 'Volume',
                  'Dividends', 'Stock Splits',
                  'Returns', 'symbol'], axis=1, inplace =True)
    try:
      self.df.drop([ 'Adj Close'], axis=1, inplace =True)
    except:
      pass
    return

  def train_test_split(self):

    X = self.df.drop(['Returns_binary'], axis=1).copy()
    y = self.df['Returns_binary'].copy()
    self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X,
                                                                            y,
                                                                            test_size=0.2,
                                                                            random_state=42)

    return




  def LR_class(self, naive=True, metric = "f1_weighted"):

    ## Running StandardScaler() for Loogistic Regression
    from sklearn import preprocessing
    scaler = preprocessing.StandardScaler().fit(self.X_train)
    X_scaled = scaler.transform(self.X_train)
    self.X_train = X_scaled

    ##run basic model
    if naive == True:
      LR_final = LogisticRegression()
      scores = cross_val_score(LR_final, self.X_train, self.y_train,
                              cv=5 , scoring=metric)
      self.LR_cv_results = scores
      print("Results of basic Logistic Regression:")
      print('Metric: ', metric, " mean score of cross val: ", scores.mean())

    else:
        ##initiation of optuna for hyperparameter tunning

        ##objective function for maximisation
        def objective(trial):

          X_train, X_val, y_train, y_val = train_test_split(self.X_train, self.y_train,
                                                        test_size=0.2,
                                                        random_state=42)

          params = {
              'tol' : trial.suggest_float('tol' , 1e-6 , 1e-3),
              'C' : trial.suggest_float("C", 1e-2, 1, log=True)}
          model = LogisticRegression(**params)
          model.fit(X_train, y_train)
          y_pred = model.predict(X_val)


          f1_result = f1_score(y_val, y_pred, average='weighted')

          return f1_result   ##f1_score(self.y_train, y_pred)

        sampler = TPESampler(seed=42)
        study = optuna.create_study(direction='maximize', sampler=sampler)
        study.optimize(objective, n_trials=100)

        ##optuna results
        print("Number of finished trials: {}".format(len(study.trials)))

        print("Best trial:")
        trial = study.best_trial

        print("  Value: {}".format(trial.value))

        print("  Params: ")
        for key, value in trial.params.items():
          print("    {}: {}".format(key, value))

        ##feed best parameters for cross validation
        params_final = study.best_trial.params
        LR_final = LogisticRegression(**params_final)


        scores = cross_val_score(LR_final, self.X_train, self.y_train,
                                cv=5 , scoring="f1_weighted")

        self.LR_cv_results = scores

        LR_final.fit(self.X_train, self.y_train)
        ##Report
        print("Results of hyper-tuned Logistic Regression:")

        ## Train results
        print('Metric: ', metric, " mean score of cross val: ", scores.mean())


        roc_train = roc_auc_score(self.y_train,
                                LR_final.predict_proba(self.X_train)[:, 1])
        Gini_train = 2*roc_train -1

        print('ROC_AUC train: ',roc_train)
        print('Gini train: ',Gini_train)
        print('F1 train (cross validation): ',scores.mean())

        ## Test results
        roc_test =roc_auc_score(self.y_test,
                                  LR_final.predict_proba(scaler.transform(self.X_test))[:, 1])
        Gini_test = 2*roc_test -1

        y_pred = LR_final.predict(scaler.transform(self.X_test))
        f1_result = f1_score(self.y_test, y_pred, average='weighted')

        print('')
        print('Results on the holdout sample')
        print('ROC_AUC test: ',roc_test)
        print('Gini test: ',Gini_test)
        print('F1-score test: ',f1_result)
        return




  def RF_class(self, naive=True, metric = "f1_weighted"):

    ##run basic model
    if naive == True:
      LR_final = RandomForestClassifier(random_state=42)
      scores = cross_val_score(LR_final, self.X_train, self.y_train,
                              cv=5 , scoring=metric)
      self.LR_cv_results = scores
      print("Results of basic Random Forest:")
      print('Metric: ', metric, " mean score of cross val: ", scores.mean())

      print(LR_final.predict_proba(self.X_train)[:, 1])
      roc_train = roc_auc_score(self.y_train,
                                LR_final.predict_proba(self.X_train)[:, 1])
      Gini_train = 2*roc_train -1

      print('ROC_AUC train: ',roc_train)
      print('Gini train: ',Gini_train)

      roc_test =roc_auc_score(self.y_test,
                                LR_final.predict_proba(scaler.transform(self.X_test))[:, 1])
      Gini_test = 2*roc_test -1

      print('ROC_AUC test: ',roc_test)
      print('Gini test: ',Gini_test)
      return
    else:
      ##initiation of optuna for hyperparameter tunning

      ##objective function for maximisation
      def objective(trial):
        ##test validation split for optuna
        X_train, X_val, y_train, y_val = train_test_split(self.X_train, self.y_train,
                                                        test_size=0.2,
                                                        random_state=42)

        params = {
            'n_estimators' : trial.suggest_int('n_estimators' , 5, 200),
            'min_samples_split': trial.suggest_int('min_samples_split', 3, 30),
            'min_samples_leaf' : trial.suggest_int('min_samples_leaf', 3, 10),
            'max_depth' : trial.suggest_int('max_depth', 3, 30),
            'bootstrap' : True,
            'random_state' : 42
        }
        model = RandomForestClassifier(**params)
        model.fit(X_train, y_train)
        y_pred = model.predict(X_val)

        f1_result = f1_score(y_val, y_pred, average='weighted')
        return f1_result

      sampler = TPESampler(seed=123)
      study = optuna.create_study(direction='maximize', sampler=sampler)
      study.optimize(objective, n_trials=100)

      ##optuna results
      print("Number of finished trials: {}".format(len(study.trials)))

      print("Best trial:")
      trial = study.best_trial

      print("  Value: {}".format(trial.value))

      print("  Params: ")
      for key, value in trial.params.items():
        print("    {}: {}".format(key, value))
      params_final = study.best_trial.params

      ##feed best parameters for cross validation
      RF_final = RandomForestClassifier(**params_final)
      RF_final = RandomForestClassifier(**params_final)


      scores = cross_val_score(RF_final, self.X_train, self.y_train,
                              cv=5, scoring="f1_weighted")

      RF_final.fit(self.X_train, self.y_train)
      ##report


      self.RF_cv_results = scores
      print("Results of hyper-tuned Random Forest:")
      print('Metric: ', metric, " mean score of cross val: ", scores.mean())
      ## Train results
      roc_train = roc_auc_score(self.y_train,
                                RF_final.predict_proba(self.X_train)[:, 1])
      Gini_train = 2*roc_train -1

      print('ROC_AUC train: ',roc_train)
      print('Gini train: ',Gini_train)

      ## Test results
      roc_test =roc_auc_score(self.y_test,
                                RF_final.predict_proba(self.X_test)[:, 1])
      Gini_test = 2*roc_test -1
      y_pred = RF_final.predict(self.X_test)
      f1_result = f1_score(self.y_test, y_pred, average='weighted')

      print('')
      print('Results on the holdout sample')
      print('ROC_AUC test: ',roc_test)
      print('Gini test: ',Gini_test)
      print('F1-score test: ',f1_result)


      ##Plot feature importance
      feature_names = self.X_train.columns
      importances = RF_final.feature_importances_
      forest_importances = pd.Series(importances, index=feature_names)

      import matplotlib.pyplot as plt
      fig, ax = plt.subplots()
      std = np.std([importances for tree in RF_final.estimators_], axis=0)
      forest_importances.plot.bar(yerr=std, ax=ax)
      ax.set_title("Feature importances for RF")
      ax.set_ylabel("Mean decrease in impurity")
      fig.tight_layout()
      plt.show()
      return

  def LightGBM(self):

    ## init dataset for LightGBM
    dtrain = lgb.Dataset(self.X_train, label=self.y_train)

    def objective(trial):


      ##test validation split for optuna
      X_train, X_val, y_train, y_val = train_test_split(self.X_train, self.y_train,
                                                        test_size=0.2,
                                                        random_state=42)

      dtrain = lgb.Dataset(X_train, label=y_train)

      ## params to optimize
      param = {
          "objective": "binary",
          "metric": "binary_logloss",
          "verbosity": -1,
          "boosting_type": "gbdt",
          "lambda_l1": trial.suggest_float("lambda_l1", 1e-8, 10.0, log=True),
          "lambda_l2": trial.suggest_float("lambda_l2", 1e-8, 10.0, log=True),
          "num_leaves": trial.suggest_int("num_leaves", 2, 256),
          "feature_fraction": trial.suggest_float("feature_fraction", 0.4, 1.0),
          "bagging_fraction": trial.suggest_float("bagging_fraction", 0.4, 1.0),
          "bagging_freq": trial.suggest_int("bagging_freq", 1, 7),
          "min_child_samples": trial.suggest_int("min_child_samples", 1, 100),
          'verbose': -1
          }
      ##fit LightGBM in trial
      gbm = lgb.train(param, dtrain)
      preds = gbm.predict(X_val)
      pred_labels = np.rint(preds)
      f1_result = f1_score(y_val, pred_labels, average='weighted')
      return f1_result



    study = optuna.create_study(direction="maximize")
    study.optimize(objective, n_trials=200)

    ##optuna results
    print("Number of finished trials: {}".format(len(study.trials)))

    print("Best trial:")
    trial = study.best_trial

    print("  Value: {}".format(trial.value))

    print("  Params: ")
    for key, value in trial.params.items():
        print("    {}: {}".format(key, value))

    self.best_LightGBM_params = trial.params


    ## CV results
    cv_results = lgb.cv(
        trial.params,
        dtrain,
        num_boost_round=100,
        nfold=5,
        metrics='f1')
    print(cv_results)
    gbm = lgb.train(trial.params, dtrain)
    pred = gbm.predict(self.X_train)

    ##Binary prediction
    pred_binary = [1 if i>0.5 else 0 for i in pred]


    f1_result =f1_score(self.y_train, pred_binary, average='weighted')


    print("Results of hyper-tuned LightGBM:")

    ## Train results
    roc_train = roc_auc_score(self.y_train, pred)
    Gini_train = 2*roc_train -1

    print('ROC_AUC train: ',roc_train)
    print('Gini train: ',Gini_train)
    print('F1-score train', f1_result)

    ## Test results
    pred = gbm.predict(self.X_test)

    ##Binary prediction
    pred_binary = [1 if i>0.5 else 0 for i in pred]

    roc_test =roc_auc_score(self.y_test, pred_binary)
    Gini_test = 2*roc_test -1

    f1_result = f1_score(self.y_test, pred_binary, average='weighted')

    print('')
    print('Results on the holdout sample')
    print('ROC_AUC test: ',roc_test)
    print('Gini test: ',Gini_test)
    print('F1-score test: ',f1_result)


    return

ML = ML_build()
ML.news_df(News_dataset) ##init with small size
##ML.load_BERT()
##ML.One_hot_encode(OHE=False)

##Import stock data and merge data on Ticker + Date
ML.import_stock_data()

ML.load_BERT()
ML.One_hot_encode(OHE=False)

## Split class in two stages

##Stage 1: download dataset
df = ML_build()
df.news_df(News_dataset.head(5)) ##init with small size
df.load_BERT()

from transformers import AutoModelForSequenceClassification
from transformers import TrainingArguments
import numpy as np
import evaluate
from transformers import AutoTokenizer
from datasets import Dataset

data = df.df_news.copy()

from datasets import Dataset
from transformers.pipelines.pt_utils import KeyDataset
from transformers import pipeline

model = AutoModelForSequenceClassification.from_pretrained("/content/drive/MyDrive/Colab Notebooks/Data_paper/Relevance_model")

tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")
nlp = pipeline("text-classification", model=model, tokenizer =tokenizer,
              device=0, batch_size=1000)


result = []

dataset = Dataset.from_pandas(df.df_news)


x = nlp(KeyDataset(dataset, 'Title'))

for value in enumerate(x):

  print("Extracted entities:")
  for entity in extracted_entities:
      print(entity)

from datasets import Dataset
from transformers.pipelines.pt_utils import KeyDataset
from transformers import pipeline

model = AutoModelForSequenceClassification.from_pretrained("/content/drive/MyDrive/Colab Notebooks/Data_paper/Relevance_model")

tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")
nlp = pipeline("text-classification", model=model, tokenizer =tokenizer,
              device=0, batch_size=1000)


result = []

dataset = Dataset.from_pandas(data)

for out in tqdm(nlp(KeyDataset(dataset, 'Title'))):
  result.append(out)
result = pd.DataFrame(result)

data= data.join(result.set_axis(data.index))

##Stage 2: prepare dataset for classification
ML = ML_build()
ML.news_df(DATA)

##encoding specific variables
ML.One_hot_encode(OHE=False)

##Import stock data and merge data on Ticker + Date
ML.import_stock_data()

ML.save_BERT()

ML.df_news

##Stage 2: prepare dataset for classification
ML = ML_build()
ML.news_df(News_dataset.head(5)) ##init with small size
ML.load_BERT()
ML.BERT_Relevance()

ML.df_news[ML.df_news['label']=='neutral']

ML.df_news

## Split class in two stages

##Stage 1: download dataset
df = ML_build()
df.news_df(News_dataset.head(5)) ##init with small size
df.load_BERT()
DATA = df.df_news

## Split class in two stages

##Stage 1: download dataset
df = ML_build()
df.news_df(News_dataset.head(5)) ##init with small size
df.load_BERT()
DATA = df.df_news


##Stage 2: prepare dataset for classification
ML = ML_build()
ML.news_df(DATA)

##encoding specific variables
ML.One_hot_encode(OHE=False)

##Import stock data and merge data on Ticker + Date
ML.import_stock_data()

## Split class in two stages

##Stage 1: download dataset
df = ML_build()
df.news_df(News_dataset.head(5)) ##init with small size
df.load_BERT()
DATA = df.df_news


##Stage 2: prepare dataset for classification
ML = ML_build()
ML.news_df(DATA)

##encoding specific variables
ML.One_hot_encode(OHE=False)

##Import stock data and merge data on Ticker + Date
ML.import_stock_data()
ML.prepare_final()

##train-test split with fixed random state = 42
ML.train_test_split()

ML.LR_class(naive=False)

ML.RF_class(naive=False)

ML.LightGBM()

ML.df.to_csv("Final_df.csv")

ML.df.describe().to_excel("Final_df_description.xlsx")

## Theoretically Full iteration of class in one run


## init class and import news df
ML = ML_build()
ML.news_df(News_dataset)

## Drop Non-english, duplicates and other
ML.data_preparation()


## Run DBSCAN model
ML.first_story_flag() ##WIP. Breaks on large dataset

## Run ML moddels
ML.BERT_sentiment_test()
ML.BERT_quality_test()
ML.BERT_NER_test()


##Save current changes to news dataframe
ML.save_BERT()
ML.load_BERT()

## Date adjustmens
ML.time_publication_adjustment() ##WIP. Non-naive (false) breaks on large dataset

##encoding specific variables
ML.One_hot_encode()

##Import stock data and merge data on Ticker + Date
ML.import_stock_data()
ML.prepare_final()

##train-test split with fixed random state = 42
ML.train_test_split()

"""####Data overveiw (Submission 2)"""

# install the sweetviz package
!pip install sweetviz
# load the sweetviz
import sweetviz
# use analyze
analyze_df = sweetviz.analyze([ML.df, "df"], target_feat = 'Returns_binary')
# then show
analyze_df.show_html('analyze.html')

"""####Discontinued functions"""

def BERT_sentiment(self, column = 'Title' ):
    from transformers import pipeline
    nlp = pipeline(task='sentiment-analysis',
               model='ProsusAI/finbert', device=0)

    def classifier(text):
      return nlp(text)
      return f'{nlp(text)}'

    ## Cool code from stackoverflow https://stackoverflow.com/questions/64342621/how-to-apply-a-sentiment-classifier-to-a-dataframe
    self.df_news = (
    self.df_news
    .assign(sentiment = lambda x: x[column].progress_apply(lambda s: classifier(s)))
    .assign(
         label = lambda x: x['sentiment'].apply(lambda s: (s[0]['label'])),
         score = lambda x: x['sentiment'].apply(lambda s: (s[0]['score']))))

def BERT_quality(self, column = 'Title' ):
    from transformers import pipeline
    nlp = pipeline("text-classification", model="ikoghoemmanuell/finetuned_fake_news_roberta",
                   device=0)

    def classifier(text):
      return nlp(text)
      return f'{nlp(text)}'

    ## Cool code from stackoverflow https://stackoverflow.com/questions/64342621/how-to-apply-a-sentiment-classifier-to-a-dataframe
    self.df_news = (
    self.df_news
    .assign(sentiment = lambda x: x[column].progress_apply(lambda s: classifier(s)))
    .assign(
         Fake = lambda x: x['sentiment'].apply(lambda s: (s[0]['label'])),
         score_fake = lambda x: x['sentiment'].apply(lambda s: (s[0]['score']))))
    return

"""## Diploma extention

Fine-tuning BERT

Groupby news by date, time
Calculate rolling CAPM
Get abnormal returns
Fine-tune BERT

# Новый раздел

###Functions
"""

ML = ML_build()
ML.news_df(News_dataset.head(5)) ##init with small size
ML.load_BERT()
ML.One_hot_encode(OHE=False)

##Import stock data and merge data on Ticker + Date
ML.import_stock_data()

ML.df_stock

ML.df_news

conf_df = ML.df_news.copy()

Pret = conf_df['FinBERT']
Fine = conf_df['label']

conf_df.columns

Fine_num = Fine.map({'positive': 0, 'negative': 1, 'neutral': 2})
Pret_num = Pret.map({'positive': 0, 'negative': 1, 'neutral': 2})

df_confusion = pd.crosstab(Fine_num, Pret_num)

df_confusion

import scipy.stats as sci

df_stock = ML.df_stock.copy()
##df_stock = df_stock.drop('Adj Close', axis=1)
df_stock.dropna()

'''def Beta_calculation(df,window, stock='Returns', market ='SP500_returns'):


  df['Beta'] = np.nan
  df['AR'] = np.nan
  df['AR_signif'] = np.nan




  risk_free = yf.Ticker('^IRX')
  risk_free = risk_free.history(period="5y")['Close'] *0.01
  risk_free =risk_free.reset_index()
  risk_free.loc[:,'Date'] = pd.to_datetime(risk_free.loc[:,'Date'])
  risk_free['Date'] = risk_free['Date'].dt.date


  for i in df['symbol'].unique():
    sliced = df[df['symbol'] == i].copy()
    covariance = sliced[stock].rolling(window).cov(sliced[market])
    variance = sliced[market].rolling(window).var()
    ##print(covariance, variance)

    beta = covariance/variance
    sliced['Beta']= beta
    if i == 'AAPL':
      print(beta)
      print(sliced)
    ##df.loc[df['symbol'] == i, 'Beta'] = beta



    ##print(sliced)
    ##print(risk_free[risk_free['Date'].isin(sliced['Date'])])
    '''try:
      sliced['Risk_free']= risk_free[risk_free['Date'].isin(sliced['Date'])]['Close'].to_list()
    except:
      sliced = sliced[sliced['Date'].isin(risk_free['Date'])]
      print(sliced)
      sliced['Risk_free']= risk_free[risk_free['Date'].isin(sliced['Date'])]['Close'].to_list()


    sliced['model_ret'] = sliced['Risk_free'] + sliced['Beta'] *( sliced[market] - sliced['Risk_free'])'''


    sliced['model_ret'] = 0 + sliced['Beta'] *( sliced[market] - 0)
    sliced['AR'] = sliced['Returns'] - sliced['model_ret']
    sliced['AR'] = sliced['AR']*100

    '''est_window = 200
    sliced['Var'] = sliced['AR'].rolling(est_window, center=True).var()
    sliced['Statistic'] = sliced['AR']/ sliced['Var']
    crit_val = sci.norm.ppf(0.99)
    def test_crit(stat, crit):
      if stat >= crit:
        return 1
      elif stat <= -crit:
        return -1
      else:
        return 0
    sliced['AR_signif'] = sliced['Statistic'].apply(lambda x: test_crit(x, crit_val))'''
    crit_val = sci.norm.ppf(0.95)
    def test_crit(stat, crit):
      if stat >= crit:
        return 1
      elif stat <= -crit:
        return 2
      else:
        return 0
    sliced['AR_signif'] = sliced['AR'].apply(lambda x: test_crit(x, crit_val))




    df.update(sliced)
  return df
df_stock = Beta_calculation(df_stock, 20)

df_stock'''

from inspect import EndOfBlock
import scipy.stats as sci

df_stock = ML.df_stock.copy()
##df_stock = df_stock.drop('Adj Close', axis=1)
##df_stock.dropna()

def Beta_calculation(df,window, stock='Returns', market ='SP500_returns'):


  symbols = df['symbol'].dropna().unique()
  df.set_index(['symbol', "Date"],  inplace=True)



  df['Beta'] = np.nan
  df['AR'] = np.nan
  df['AR_signif'] = np.nan

  est_window = 50
  event_window = 10

  df['Event'] = np.nan
  df['Event_ID'] = np.nan





  risk_free = yf.Ticker('^IRX')
  risk_free = risk_free.history(period="5y")['Close'] *0.01
  risk_free =risk_free.reset_index()
  risk_free.loc[:,'Date'] = pd.to_datetime(risk_free.loc[:,'Date'])
  risk_free['Date'] = risk_free['Date'].dt.date

  unique_ID = 0
  for i in symbols:

    unique_ID = round(unique_ID/100)*100
    unique_ID += 100


    print(i)
    sliced_by_company = df.loc[i].copy()
    sliced_by_company.reset_index(inplace=True)



    print(sliced_by_company.index[sliced_by_company['Reported EPS'].notna()].tolist())
    ##print(sliced_by_company[sliced_by_company['Reported EPS'].notna()])

    for event in sliced_by_company.index[sliced_by_company['Surprise(%)'].notna()].tolist():

      unique_ID += 1

      Start_of_slice = int(event- event_window/2 -est_window)
      End_of_slice = int(event+ event_window/2)

      ##print(Start_of_slice,int(event-event_window/2),  End_of_slice)
      sliced = sliced_by_company[Start_of_slice : End_of_slice].copy()

      sliced["Event_ID"] = unique_ID
      sliced.loc[int(event-event_window/2)  : End_of_slice, "Event" ] = 1

      stock_var = sliced.loc[Start_of_slice: int(event-event_window/2), stock]
      market_var = sliced.loc[Start_of_slice: int(event-event_window/2), market]

      '''covariance = sliced[stock].cov(sliced[market])
      variance = sliced[market].var()'''

      covariance = stock_var.cov(market_var)
      variance = market_var.var()

      ##print(covariance, variance)

      beta = covariance/variance
      sliced['Beta']= beta
      if i == 'AAPL':
        print(beta)
        ##print(sliced)
      ##df.loc[df['symbol'] == i, 'Beta'] = beta



      ##print(sliced)
      ##print(risk_free[risk_free['Date'].isin(sliced['Date'])])
      '''sliced['Risk_free']= risk_free[risk_free['Date'].isin(sliced['Date'])]['Close'].copy()
      try:
        sliced['Risk_free']= risk_free[risk_free['Date'].isin(sliced['Date'])]['Close'].to_list()
      except:
        ##print(sliced['Risk_free'])
        print(len(sliced['Date']))
        sliced = sliced[sliced['Date'].isin(risk_free['Date'])]
        print(len(sliced['Date']))

        sliced['Risk_free']= risk_free[risk_free['Date'].isin(sliced['Date'])]['Close'].to_list()'''

      ## CAPM with risk-free
      ##sliced['model_ret'] = sliced['Risk_free'] + sliced['Beta'] *( sliced[market] - sliced['Risk_free'])

      sliced['model_ret'] = 0 + sliced['Beta'] *( sliced[market] - 0)
      sliced['AR'] = sliced['Returns'] - sliced['model_ret']
      sliced['AR'] = sliced['AR']*100



      crit_val = sci.norm.ppf(0.95)
      def test_crit(stat, crit):
        if stat >= crit:
          return 1
        elif stat <= -crit:
          return 2
        else:
          return 0
      sliced['AR_signif'] = sliced['AR'].apply(lambda x: test_crit(x, crit_val))

      sliced_by_company.update(sliced, overwrite=False)


      if i == 'AAPL':
        test_df = df.copy()

      print(unique_ID)

    sliced_by_company['symbol'] = i
    sliced_by_company.set_index(['symbol', 'Date'], inplace=True)

    try:
      df.update(sliced_by_company, overwrite=False)
    except:
      print(sliced_by_company)


  df.reset_index(inplace= True)
  return df, test_df
df_stock, test_df = Beta_calculation(df_stock, 30)

df_stock.to_csv('df_stock.csv', index=True)

df_stock = pd.read_csv("/content/df_stock.csv")
df_stock['Date'] = pd.to_datetime(df_stock.loc[:,'Date'])
df_stock['Date'] = df_stock['Date'] .dt.date

News_df = ML.df_news.copy()

News_df[0].columns

New_cols = ["N_", "n_", "P_", "L0_", "L1_", "RN_", "Rn_", "RP_" ]
old_cols = ['negative', 'neutral', 'positive', 'LABEL_0', 'LABEL_1', 'Rel_neg',
       'Rel_neut', 'Rel_pos']
for lag in [20, 40, 60, 120]:
  names = [ i + str(lag) for i in New_cols]
  print(names)
  News_df[names] = News_df[old_cols].rolling(window=lag).sum()

df_stock

def prepare_news_df( curated= True, Organisation = True):
  News_df = ML.df_news.copy()
  News_df =News_df.reset_index()

  Date_col= pd.to_datetime(News_df['Date'], utc=True, errors='coerce')
  News_df['Date'] = Date_col.dt.date





  News_df['positive*Label_0'] = News_df['positive'] * News_df['LABEL_0']
  News_df['negative*Label_0'] = News_df['negative'] * News_df['LABEL_0']


  ##Relevance adjustment
  News_df['Rel_pos*Label_1'] = News_df['Rel_pos'] * News_df['LABEL_1']
  News_df['Rel_neg*Label_1'] = News_df['Rel_neg'] * News_df['LABEL_1']
  News_df['Rel_pos*Label_0'] = News_df['Rel_pos'] * News_df['LABEL_0']
  News_df['Rel_neg*Label_0'] = News_df['Rel_neg'] * News_df['LABEL_0']

  Original_News_df = News_df.copy()

  if Organisation == True:
    News_df = News_df[News_df['B-ORG'] == 1]

  if curated==True:
    sources_df = pd.read_excel("/content/Sources_class.xlsx", header=1).fillna(0)

    News_df = News_df.merge(sources_df, on="Source", how='left')
    News_df =News_df.drop("Source", axis=1)
    News_df =News_df.drop("count", axis=1)
    News_df = News_df[(News_df['Relevant'] >0) | (News_df['Traditional'] >0)]

  News_df = News_df[["Date", "Company", 'positive', 'negative',
                    'positive*Label_1', 'negative*Label_1', 'positive*Label_0','negative*Label_0',
                    'Rel_pos*Label_1', 'Rel_neg*Label_1', 'Rel_pos*Label_0', 'Rel_neg*Label_0']]


  News_df= News_df.groupby(['Company', 'Date']).sum()

  return News_df, Original_News_df

def merge_stock_news(news_df, stock_df, only_rel =True, no_Events=False ):

  df_stock = pd.merge(stock_df, news_df.reset_index(), left_on=['symbol', 'Date'],
                       right_on=['Company', 'Date'], how="left")


  print(df_stock)
  df_stock = df_stock[df_stock['Date'] >= pd.to_datetime('2020-01-01').date() ]
  df_stock = df_stock[df_stock['Date'] <= pd.to_datetime('2023-09-01').date() ]


  ## Market uncertanty index

  Uncertainty_df = pd.DataFrame()
  for date in df_stock.Date.unique():
    Set = df_stock[df_stock.Date == date]
    Positive_sum = Set['positive'].sum()
    Negative_sum = Set['negative'].sum()
    try:

      Uncert = max(0, 1- Negative_sum / (Positive_sum + Negative_sum))
      ##print(Positive_sum, Negative_sum, Uncert)

    except:
      Uncert = 0
    df_stock.loc[df_stock.Date == date, 'Uncert']  = Uncert
    day_dict = {"Date": date, "Uncert": Uncert}
    Uncertainty_df = pd.concat([Uncertainty_df, pd.DataFrame([day_dict])], ignore_index=True)
    ##Uncertainty_df = Uncertainty_df.append(day_dict)

  print(Uncertainty_df)

  ## Event window

  try:
    df_stock["Event_only_index"] =np.nan
    df_stock.loc[(df_stock["Event"]==1) & (df_stock["Event_ID"].notna()), "Event_only_index"] = df_stock["Event_ID"]

  except:
    print('Error in EVENT_ID column')


  ## Amihud computation

  df_stock["Illiquidity"] = abs(df_stock["Returns"])/(df_stock["High"] * df_stock["High"]*df_stock["Volume"]/2)

  grouped = df_stock.drop("Date", axis=1).groupby('symbol')
  print(grouped)


  results = {'symbol':[], 'illiquidity':[]}
  for item, grp in grouped:
    print(item)
    ##print(grp.tail(2))

    ##subset_mean = grp.tail(2).sum()[0]
    subset_mean =grp["Illiquidity"].mean()
    print(subset_mean)
    results['symbol'].append(item)
    results['illiquidity'].append(subset_mean)

  print(results)
  res_df = pd.DataFrame(results)
  Quant = res_df["illiquidity"].quantile(0.75)
  res_df['Illiq'] = res_df['illiquidity'].ge(Quant).astype(int)
  df_stock = df_stock.merge(res_df, on='symbol', how='left')


  ##Sentiment index
  New_cols = ["PL1_", "NL1_", "PL0_", "NL0_", "RPL1_","RNL1_", "RPL0_", "RNL0_"]
  old_cols = ['positive*Label_1', 'negative*Label_1', 'positive*Label_0', 'negative*Label_0', 'Rel_pos*Label_1', 'Rel_neg*Label_1',
        'Rel_pos*Label_0', 'Rel_neg*Label_0']
  for lag in [20, 40, 60, 120]:
    names = [ i + str(lag) for i in New_cols]
    print(names)
    df_stock[names] = df_stock[old_cols].rolling(window=lag).sum()

  df_stock.reset_index(inplace=True)



  if no_Events == True:

    df_stock["Date"] = pd.to_datetime(df_stock["Date"])

    test = df_stock[["Date",  'Volume', 'Dividends', 'Stock Splits', 'Market_Cap', 'SP500_returns', 'SP500_returns_yesterday', 'Returns', 'AR', 'EPS Estimate', 'Reported EPS',
       'Surprise(%)', 'symbol', 'positive*Label_1',
       'negative*Label_1', 'positive*Label_0', 'negative*Label_0', 'Uncert',
       'Event_only_index', 'Rel_pos*Label_1', 'Rel_neg*Label_1',
       'Rel_pos*Label_0', 'Rel_neg*Label_0', "Illiq",'PL1_20', 'NL1_20', 'PL0_20',
       'NL0_20', 'RPL1_20', 'RNL1_20', 'RPL0_20', 'RNL0_20', 'PL1_40',
       'NL1_40', 'PL0_40', 'NL0_40', 'RPL1_40', 'RNL1_40', 'RPL0_40',
       'RNL0_40', 'PL1_60', 'NL1_60', 'PL0_60', 'NL0_60', 'RPL1_60', 'RNL1_60',
       'RPL0_60', 'RNL0_60', 'PL1_120', 'NL1_120', 'PL0_120', 'NL0_120',
       'RPL1_120', 'RNL1_120', 'RPL0_120', 'RNL0_120']]
    model_run = test.groupby(['symbol', 'Date']).sum()
    ##return model_run, df_stock

  else:

    test = df_stock[['Date', 'Volume', 'Dividends', 'Stock Splits', 'Market_Cap', 'SP500_returns', 'SP500_returns_yesterday', 'Returns', 'AR', 'EPS Estimate', 'Reported EPS',
        'Surprise(%)', 'symbol', 'Event', 'Event_ID', 'positive*Label_1',
        'negative*Label_1', 'positive*Label_0', 'negative*Label_0', 'Uncert',
        'Event_only_index', 'Rel_pos*Label_1', 'Rel_neg*Label_1',
        'Rel_pos*Label_0', 'Rel_neg*Label_0', "Illiq",'PL1_20', 'NL1_20', 'PL0_20',
        'NL0_20', 'RPL1_20', 'RNL1_20', 'RPL0_20', 'RNL0_20', 'PL1_40',
        'NL1_40', 'PL0_40', 'NL0_40', 'RPL1_40', 'RNL1_40', 'RPL0_40',
        'RNL0_40', 'PL1_60', 'NL1_60', 'PL0_60', 'NL0_60', 'RPL1_60', 'RNL1_60',
        'RPL0_60', 'RNL0_60', 'PL1_120', 'NL1_120', 'PL0_120', 'NL0_120',
        'RPL1_120', 'RNL1_120', 'RPL0_120', 'RNL0_120']]

    test2 = test.drop(test[test['Event_only_index'] == 0.0].index)
    ##model_run = test2.groupby(['symbol', 'Event_only_index']).sum()

    ##text_cols = df.columns[1:3]
    data_cols = set(test2.columns) - set(["Date", 'symbol'])
    d1 = dict.fromkeys(data_cols, 'sum')
    d2 = dict.fromkeys(["Date"], 'first')
    d = {**d1, **d2}

    model_run = test2.groupby(['symbol', 'Event_only_index']).agg(d)
    print(model_run)

    ## Fill in uncertainty index
    model_run.drop("Uncert", axis=1, inplace=True)

    def get_uncertainty(df, df_uncert):

      date = df['Date']
      Index_value = df_uncert[df_uncert['Date'] == date]['Uncert'].values[0]
      print(date, Index_value)
      return Index_value

    model_run['Uncert']= model_run.apply(lambda x: get_uncertainty(x, Uncertainty_df), axis =1)
  ##model_run= model_run.merge(Uncertainty_df, how='left', on=['Date'])
  print(model_run)
  ##model_run.set_index(['symbol', 'Event_only_index'], inplace=True)




  ##functions for other columns
  def new_dummy(df, key_col, col_to_apply):
    for i in col_to_apply:
      name = i +"_" + key_col
      print(name)
      df[name] = df[key_col]* df[i]
    return df


  def new_index(df, Positive, all_cols, scale=10):
    for i in Positive:
      name = i +"_" + "index"
      print(name)

      Index_of_Negative = all_cols.index(i) +1
      neg_col = all_cols[Index_of_Negative]
      df[name] = (df[i] - df[neg_col])/ (df[i] + df[neg_col])/10
    return df

  Lag_list = ['PL1_20', 'NL1_20',
       'PL0_20', 'NL0_20', 'RPL1_20', 'RNL1_20', 'RPL0_20', 'RNL0_20',
       'PL1_40', 'NL1_40', 'PL0_40', 'NL0_40', 'RPL1_40', 'RNL1_40', 'RPL0_40',
       'RNL0_40', 'PL1_60', 'NL1_60', 'PL0_60', 'NL0_60', 'RPL1_60', 'RNL1_60',
       'RPL0_60', 'RNL0_60', 'PL1_120', 'NL1_120', 'PL0_120', 'NL0_120',
       'RPL1_120', 'RNL1_120', 'RPL0_120', 'RNL0_120']

  Positive_values = ['PL1_20', 'PL0_20',  'RPL1_20',  'RPL0_20',
        'PL1_40', 'PL0_40',  'RPL1_40', 'RPL0_40', 'PL1_60', 'PL0_60', 'RPL1_60',
        'RPL0_60', 'PL1_120', 'PL0_120', 'RPL1_120',  'RPL0_120']
  Negative_values = list(set(Lag_list) - set(Positive_values))
  model_run = new_index(model_run, Positive_values, Lag_list)



  index_list =['PL1_20_index', 'PL0_20_index','RPL1_20_index', 'RPL0_20_index',
               'PL1_40_index', 'PL0_40_index', 'RPL1_40_index', 'RPL0_40_index',
               'PL1_60_index', 'PL0_60_index', 'RPL1_60_index', 'RPL0_60_index',
               'PL1_120_index', 'PL0_120_index','RPL1_120_index', 'RPL0_120_index']

  model_run = new_dummy(model_run, "Uncert", index_list)
  model_run = new_dummy(model_run, "Illiq", index_list)


  Col_list = ['positive*Label_1', 'negative*Label_1', 'positive*Label_0',
              'negative*Label_0', 'Rel_pos*Label_1', 'Rel_neg*Label_1',
              'Rel_pos*Label_0', 'Rel_neg*Label_0']
  model_run = new_dummy(model_run, "Uncert", Col_list)
  model_run = new_dummy(model_run, "Illiq", Col_list)




  return model_run, df_stock

News_df = prepare_news_df(curated= True, Organisation = True)

model_run = merge_stock_news(News_df[0], df_stock, only_rel=False, no_Events=False )

##Legacy code. Do not run


def prepare_news_df( curated= True, Organisation = True):
  News_df = ML.df_news.copy()
  News_df =News_df.reset_index()

  Date_col= pd.to_datetime(News_df['Date'], utc=True, errors='coerce')
  News_df['Date'] = Date_col.dt.date





  News_df['positive*Label_0'] = News_df['positive'] * News_df['LABEL_0']
  News_df['negative*Label_0'] = News_df['negative'] * News_df['LABEL_0']


  ##Relevance adjustment
  News_df['Rel_pos*Label_1'] = News_df['Rel_pos'] * News_df['LABEL_1']
  News_df['Rel_neg*Label_1'] = News_df['Rel_neg'] * News_df['LABEL_1']
  News_df['Rel_pos*Label_0'] = News_df['Rel_pos'] * News_df['LABEL_0']
  News_df['Rel_neg*Label_0'] = News_df['Rel_neg'] * News_df['LABEL_0']

  Original_News_df = News_df.copy()

  if Organisation == True:
    News_df = News_df[News_df['B-ORG'] == 1]

  if curated==True:
    sources_df = pd.read_excel("/content/Sources_class.xlsx", header=1).fillna(0)

    News_df = News_df.merge(sources_df, on="Source", how='left')
    News_df =News_df.drop("Source", axis=1)
    News_df =News_df.drop("count", axis=1)
    News_df = News_df[(News_df['Relevant'] >0) | (News_df['Traditional'] >0)]

  News_df = News_df[["Date", "Company", 'positive', 'negative',
                    'positive*Label_1', 'negative*Label_1', 'positive*Label_0','negative*Label_0',
                    'Rel_pos*Label_1', 'Rel_neg*Label_1', 'Rel_pos*Label_0', 'Rel_neg*Label_0']]


  News_df= News_df.groupby(['Company', 'Date']).sum()

  return News_df, Original_News_df

def merge_stock_news(news_df, stock_df, only_rel =True, no_Events=False ):

  df_stock = pd.merge(stock_df, news_df.reset_index(), left_on=['symbol', 'Date'],
                       right_on=['Company', 'Date'], how="left")


  print(df_stock)
  df_stock = df_stock[df_stock['Date'] >= pd.to_datetime('2020-01-01').date() ]
  df_stock = df_stock[df_stock['Date'] <= pd.to_datetime('2023-09-01').date() ]


  ## Market uncertanty index

  Uncertainty_df = pd.DataFrame()
  for date in df_stock.Date.unique():
    Set = df_stock[df_stock.Date == date]
    Positive_sum = Set['positive'].sum()
    Negative_sum = Set['negative'].sum()
    try:

      Uncert = max(0, 1- Negative_sum / (Positive_sum + Negative_sum))
      ##print(Positive_sum, Negative_sum, Uncert)

    except:
      Uncert = 0
    df_stock.loc[df_stock.Date == date, 'Uncert']  = Uncert
    day_dict = {"Date": date, "Uncert": Uncert}
    Uncertainty_df = pd.concat([Uncertainty_df, pd.DataFrame([day_dict])], ignore_index=True)
    ##Uncertainty_df = Uncertainty_df.append(day_dict)

  print(Uncertainty_df)

  ## Event window

  try:
    df_stock["Event_only_index"] =np.nan
    df_stock.loc[(df_stock["Event"]==1) & (df_stock["Event_ID"].notna()), "Event_only_index"] = df_stock["Event_ID"]

  except:
    print('Error in EVENT_ID column')


  ## Amihud computation

  df_stock["Illiquidity"] = abs(df_stock["Returns"])/(df_stock["High"] * df_stock["High"]*df_stock["Volume"]/2)

  grouped = df_stock.drop("Date", axis=1).groupby('symbol')
  print(grouped)


  results = {'symbol':[], 'illiquidity':[]}
  for item, grp in grouped:
    print(item)
    ##print(grp.tail(2))

    ##subset_mean = grp.tail(2).sum()[0]
    subset_mean =grp["Illiquidity"].mean()
    print(subset_mean)
    results['symbol'].append(item)
    results['illiquidity'].append(subset_mean)

  print(results)
  res_df = pd.DataFrame(results)
  Quant = res_df["illiquidity"].quantile(0.75)
  res_df['Illiq'] = res_df['illiquidity'].ge(Quant).astype(int)
  df_stock = df_stock.merge(res_df, on='symbol', how='left')


  ##Sentiment index
  New_cols = ["PL1_", "NL1_", "PL0_", "NL0_", "RPL1_","RNL1_", "RPL0_", "RNL0_"]
  old_cols = ['positive*Label_1', 'negative*Label_1', 'positive*Label_0', 'negative*Label_0', 'Rel_pos*Label_1', 'Rel_neg*Label_1',
        'Rel_pos*Label_0', 'Rel_neg*Label_0']
  for lag in [20, 40, 60, 120]:
    names = [ i + str(lag) for i in New_cols]
    print(names)
    df_stock[names] = df_stock[old_cols].rolling(window=lag).sum()

  df_stock.reset_index(inplace=True)



  if no_Events == True:

    df_stock["Date"] = pd.to_datetime(df_stock["Date"])

    test = df_stock[["Date",  'Volume', 'Dividends', 'Stock Splits', 'Market_Cap', 'SP500_returns', 'SP500_returns_yesterday', 'Returns', 'AR', 'EPS Estimate', 'Reported EPS',
       'Surprise(%)', 'symbol', 'positive*Label_1',
       'negative*Label_1', 'positive*Label_0', 'negative*Label_0', 'Uncert',
       'Event_only_index', 'Rel_pos*Label_1', 'Rel_neg*Label_1',
       'Rel_pos*Label_0', 'Rel_neg*Label_0', "Illiq",'PL1_20', 'NL1_20', 'PL0_20',
       'NL0_20', 'RPL1_20', 'RNL1_20', 'RPL0_20', 'RNL0_20', 'PL1_40',
       'NL1_40', 'PL0_40', 'NL0_40', 'RPL1_40', 'RNL1_40', 'RPL0_40',
       'RNL0_40', 'PL1_60', 'NL1_60', 'PL0_60', 'NL0_60', 'RPL1_60', 'RNL1_60',
       'RPL0_60', 'RNL0_60', 'PL1_120', 'NL1_120', 'PL0_120', 'NL0_120',
       'RPL1_120', 'RNL1_120', 'RPL0_120', 'RNL0_120']]
    model_run = test.groupby(['symbol', 'Date']).sum()
    ##return model_run, df_stock

  else:

    test = df_stock[['Date', 'Volume', 'Dividends', 'Stock Splits', 'Market_Cap', 'SP500_returns', 'SP500_returns_yesterday', 'Returns', 'AR', 'EPS Estimate', 'Reported EPS',
        'Surprise(%)', 'symbol', 'Event', 'Event_ID', 'positive*Label_1',
        'negative*Label_1', 'positive*Label_0', 'negative*Label_0', 'Uncert',
        'Event_only_index', 'Rel_pos*Label_1', 'Rel_neg*Label_1',
        'Rel_pos*Label_0', 'Rel_neg*Label_0', "Illiq",'PL1_20', 'NL1_20', 'PL0_20',
        'NL0_20', 'RPL1_20', 'RNL1_20', 'RPL0_20', 'RNL0_20', 'PL1_40',
        'NL1_40', 'PL0_40', 'NL0_40', 'RPL1_40', 'RNL1_40', 'RPL0_40',
        'RNL0_40', 'PL1_60', 'NL1_60', 'PL0_60', 'NL0_60', 'RPL1_60', 'RNL1_60',
        'RPL0_60', 'RNL0_60', 'PL1_120', 'NL1_120', 'PL0_120', 'NL0_120',
        'RPL1_120', 'RNL1_120', 'RPL0_120', 'RNL0_120']]

    test2 = test.drop(test[test['Event_only_index'] == 0.0].index)
    ##model_run = test2.groupby(['symbol', 'Event_only_index']).sum()

    ##text_cols = df.columns[1:3]
    data_cols = set(test2.columns) - set(["Date", 'symbol'])
    d1 = dict.fromkeys(data_cols, 'sum')
    d2 = dict.fromkeys(["Date"], 'first')
    d = {**d1, **d2}

    model_run = test2.groupby(['symbol', 'Event_only_index']).agg(d)
    print(model_run)

    ## Fill in uncertainty index
    model_run.drop("Uncert", axis=1, inplace=True)

    def get_uncertainty(df, df_uncert):

      date = df['Date']
      Index_value = df_uncert[df_uncert['Date'] == date]['Uncert'].values[0]
      print(date, Index_value)
      return Index_value

    model_run['Uncert']= model_run.apply(lambda x: get_uncertainty(x, Uncertainty_df), axis =1)
  ##model_run= model_run.merge(Uncertainty_df, how='left', on=['Date'])
  print(model_run)
  ##model_run.set_index(['symbol', 'Event_only_index'], inplace=True)




  ##functions for other columns
  def new_dummy(df, key_col, col_to_apply):
    for i in col_to_apply:
      name = i +"_" + key_col
      print(name)
      df[name] = df[key_col]* df[i]
    return df


  def new_index(df, Positive, all_cols, scale=10):
    for i in Positive:
      name = i +"_" + "index"
      print(name)

      Index_of_Negative = all_cols.index(i) +1
      neg_col = all_cols[Index_of_Negative]
      df[name] = (df[i] - df[neg_col])/ (df[i] + df[neg_col])/10
    return df

  Lag_list = ['PL1_20', 'NL1_20',
       'PL0_20', 'NL0_20', 'RPL1_20', 'RNL1_20', 'RPL0_20', 'RNL0_20',
       'PL1_40', 'NL1_40', 'PL0_40', 'NL0_40', 'RPL1_40', 'RNL1_40', 'RPL0_40',
       'RNL0_40', 'PL1_60', 'NL1_60', 'PL0_60', 'NL0_60', 'RPL1_60', 'RNL1_60',
       'RPL0_60', 'RNL0_60', 'PL1_120', 'NL1_120', 'PL0_120', 'NL0_120',
       'RPL1_120', 'RNL1_120', 'RPL0_120', 'RNL0_120']

  Positive_values = ['PL1_20', 'PL0_20',  'RPL1_20',  'RPL0_20',
        'PL1_40', 'PL0_40',  'RPL1_40', 'RPL0_40', 'PL1_60', 'PL0_60', 'RPL1_60',
        'RPL0_60', 'PL1_120', 'PL0_120', 'RPL1_120',  'RPL0_120']
  Negative_values = list(set(Lag_list) - set(Positive_values))
  model_run = new_index(model_run, Positive_values, Lag_list)



  index_list =['PL1_20_index', 'PL0_20_index','RPL1_20_index', 'RPL0_20_index',
               'PL1_40_index', 'PL0_40_index', 'RPL1_40_index', 'RPL0_40_index',
               'PL1_60_index', 'PL0_60_index', 'RPL1_60_index', 'RPL0_60_index',
               'PL1_120_index', 'PL0_120_index','RPL1_120_index', 'RPL0_120_index']

  model_run = new_dummy(model_run, "Uncert", index_list)
  model_run = new_dummy(model_run, "Illiq", index_list)


  Col_list = ['positive*Label_1', 'negative*Label_1', 'positive*Label_0',
              'negative*Label_0', 'Rel_pos*Label_1', 'Rel_neg*Label_1',
              'Rel_pos*Label_0', 'Rel_neg*Label_0']
  model_run = new_dummy(model_run, "Uncert", Col_list)
  model_run = new_dummy(model_run, "Illiq", Col_list)




  return model_run, df_stock

News_df = prepare_news_df(curated= True, Organisation = True)

model_run = merge_stock_news(News_df[0], df_stock, only_rel=False, no_Events=False )

News_df[0]

model_run[0]

model_run[0]

model_run[0]

model_run[0].describe()

!pip install linearmodels -q
import linearmodels as lm
model_run[0]['const'] = 1
model_run[0]['Market_Cap'] = np.log(model_run[0]['Market_Cap'])
model_run[0]['Volume'] = np.log(model_run[0]['Volume'])

model_run[0].describe()

model_run[0].describe().to_excel('H0_description.xlsx')

for i in model_run[0].columns:
  print(i)

endog = model_run[0]['AR']

exogen = model_run[0][['positive*Label_1','negative*Label_1',
                       'Surprise(%)', 'Dividends', 'Market_Cap']]
m = lm.PanelOLS(endog,exogen, entity_effects=True, check_rank=False, drop_absorbed=True)
H1  = m.fit(cov_type='clustered', cluster_entity=True, )

print(H1)
exogen = model_run[0][['positive*Label_1','negative*Label_1',
                       'positive*Label_0',
                       'Surprise(%)', 'Dividends', 'Market_Cap',  'const']]
m = lm.PanelOLS(endog,exogen, entity_effects=True, check_rank=False, drop_absorbed=True)
H1  = m.fit(cov_type='clustered', cluster_entity=True, )

print(H1)

'''exogen = model_run[0][['positive*Label_1','negative*Label_1’, ‘negative*Label_0',
                       'Surprise(%)', 'Dividends', 'Market_Cap',  'const']]
m = lm.PanelOLS(endog,exogen, entity_effects=True, check_rank=False, drop_absorbed=True)
H1  = m.fit(cov_type='clustered', cluster_entity=True, )

print(H1)'''


exogen = model_run[0][['positive*Label_1','negative*Label_1',
                       'positive*Label_0','negative*Label_0',
                       'Surprise(%)', 'Dividends', 'Market_Cap', 'const']]
m = lm.PanelOLS(endog,exogen, entity_effects=True, check_rank=False, drop_absorbed=True)
H1  = m.fit(cov_type='clustered', cluster_entity=True, )

print(H1)

endog = model_run[0]['AR']

exogen = model_run[0][['positive*Label_1','negative*Label_1',
                       'positive*Label_0','negative*Label_0',
                       'Surprise(%)', 'Dividends', 'Market_Cap', 'Volume',  'const']]
m = lm.PanelOLS(endog,exogen, entity_effects=True, check_rank=False, drop_absorbed=True)
H1  = m.fit(cov_type='clustered', cluster_entity=True, )

print(H1)

exogen = model_run[0][['positive*Label_1','negative*Label_1',
                       'positive*Label_0','negative*Label_0',
                       'positive*Label_1_Uncert',
                       'negative*Label_1_Uncert', 'positive*Label_0_Uncert',
                       'negative*Label_0_Uncert', 'Dividends', 'Market_Cap', 'Volume',  'const']]
m = lm.PanelOLS(endog,exogen, entity_effects=True)
H2  = m.fit(cov_type='clustered', cluster_entity=True)

print(H2)


exogen = model_run[0][['positive*Label_1','negative*Label_1',
                       'positive*Label_0','negative*Label_0',
                       'Surprise(%)', 'positive*Label_1_Illiq',
                       'negative*Label_1_Illiq', 'positive*Label_0_Illiq',
                       'negative*Label_0_Illiq',  'Market_Cap', 'Volume', 'const']]
m = lm.PanelOLS(endog,exogen, entity_effects=True)
H3  = m.fit(cov_type='clustered', cluster_entity=True)

print(H3)

for i in model_run[0].columns:
  print(i)

model_run[0]

News_df[1][['Title','Company', 'B-ORG']]

!pip install yahoo_fin

model_run[1][["Date", "symbol", "Returns", 'SP500_returns', "Beta", "AR"]]

model_run[1].columns

'Rel_pos*Label_1_Illiq', 'Rel_neg*Label_1_Illiq',
       'Rel_pos*Label_0_Illiq', 'Rel_neg*Label_0_Illiq'

!pip install linearmodels -q
import linearmodels as lm
model_run[0]['const'] = 1
model_run[0]['Market_Cap'] = np.log(model_run[0]['Market_Cap'])
model_run[0]['Volume'] = np.log(model_run[0]['Volume'])

!pip install Stargazer

import Stargazer

model_run[0]['Volume']

model_run[0]['Market_Cap']

model_run[0][['Rel_pos*Label_1','Rel_neg*Label_1',
                       'Rel_pos*Label_0','Rel_neg*Label_0',
                       'Surprise(%)', 'Uncert', 'Rel_pos*Label_1_Uncert',
                       'Rel_neg*Label_1_Uncert', 'Rel_pos*Label_0_Uncert',
                       'Rel_neg*Label_0_Uncert', 'Surprise(%)', 'const']]

endog = model_run[0]['AR']

exogen = model_run[0][['positive*Label_1','negative*Label_1',
                       'positive*Label_0','negative*Label_0',
                       'Surprise(%)', 'Dividends', 'Market_Cap', 'Volume',  'const']]
m = lm.PanelOLS(endog,exogen, entity_effects=True, check_rank=False, drop_absorbed=True)
H1  = m.fit(cov_type='clustered', cluster_entity=True, )

print(H1)

exogen = model_run[0][['positive*Label_1','negative*Label_1',
                       'positive*Label_0','negative*Label_0',
                       'positive*Label_1_Uncert',
                       'negative*Label_1_Uncert', 'positive*Label_0_Uncert',
                       'negative*Label_0_Uncert', 'Dividends', 'Market_Cap', 'Volume',  'const']]
m = lm.PanelOLS(endog,exogen, entity_effects=True)
H2  = m.fit(cov_type='clustered', cluster_entity=True)

print(H2)


exogen = model_run[0][['positive*Label_1','negative*Label_1',
                       'positive*Label_0','negative*Label_0',
                       'Surprise(%)', 'positive*Label_1_Illiq',
                       'negative*Label_1_Illiq', 'positive*Label_0_Illiq',
                       'negative*Label_0_Illiq',  'Market_Cap', 'Volume', 'const']]
m = lm.PanelOLS(endog,exogen, entity_effects=True)
H3  = m.fit(cov_type='clustered', cluster_entity=True)

print(H3)

model_run_final = model_run[0].copy()
model_run_final.columns = model_run[0].columns.str.lower().str.replace('*','_').str.replace('(%)','_percent')

model_run_final

m = lm.PanelOLS.from_formula('ar ~ const +   dividends + volume + market_cap+ surprise_percent + positive_label_1 + negative_label_1 + EntityEffects', data = model_run_final, check_rank=False, drop_absorbed=True)
H100  = m.fit(cov_type='clustered', cluster_entity=True )
print(H100)

m = lm.PanelOLS.from_formula('ar ~ const +   dividends + volume + market_cap+ surprise_percent + positive_label_1 + negative_label_1 + positive_label_0  + EntityEffects', data = model_run_final, check_rank=False, drop_absorbed=True)
H101  = m.fit(cov_type='clustered', cluster_entity=True )
print(H101)

m = lm.PanelOLS.from_formula('ar ~ const +   dividends + volume + market_cap+ surprise_percent + positive_label_1 + negative_label_1 + negative_label_0 + EntityEffects', data = model_run_final, check_rank=False, drop_absorbed=True)
H102  = m.fit(cov_type='clustered', cluster_entity=True )
print(H102)

m = lm.PanelOLS.from_formula('ar ~ const +   dividends  + market_cap+ volume + surprise_percent + positive_label_1 + negative_label_1 + positive_label_0 + negative_label_0 + EntityEffects', data = model_run_final, check_rank=False, drop_absorbed=True)
H103  = m.fit(cov_type='clustered', cluster_entity=True )
print(H103)



m = lm.PanelOLS.from_formula('ar ~ const +   dividends + volume + market_cap+ surprise_percent + rel_pos_label_1 + rel_neg_label_1  + EntityEffects', data = model_run_final, check_rank=False, drop_absorbed=True)
H104  = m.fit(cov_type='clustered', cluster_entity=True )
print(H104)


m = lm.PanelOLS.from_formula('ar ~ const +   dividends + volume + market_cap+ surprise_percent+ rel_pos_label_1 + rel_neg_label_1 + rel_pos_label_0 + EntityEffects', data = model_run_final, check_rank=False, drop_absorbed=True)
H105  = m.fit(cov_type='clustered', cluster_entity=True )
print(H105)


m = lm.PanelOLS.from_formula('ar ~ const +   dividends + volume + market_cap+ surprise_percent+ rel_pos_label_1 + rel_neg_label_1 + rel_neg_label_0 + EntityEffects', data = model_run_final, check_rank=False, drop_absorbed=True)
H106  = m.fit(cov_type='clustered', cluster_entity=True )
print(H106)

m = lm.PanelOLS.from_formula('ar ~ const +   dividends + volume + market_cap+ surprise_percent + rel_pos_label_1 + rel_neg_label_1 + rel_pos_label_0 + rel_neg_label_0 + EntityEffects', data = model_run_final, check_rank=False, drop_absorbed=True)
H107  = m.fit(cov_type='clustered', cluster_entity=True )
print(H107)



report = Stargazer([H100, H101, H102, H103, H104, H105, H106, H107])
##report.render_html()
with open("Report0.html", "w") as file:
    file.write(report.render_html())

m = lm.PanelOLS.from_formula('ar ~ const +   dividends + volume + market_cap+ surprise_percent + positive_label_1 + negative_label_1 + EntityEffects', data = model_run_final, check_rank=False, drop_absorbed=True)
H100  = m.fit(cov_type='clustered', cluster_entity=True )
print(H100)

m = lm.PanelOLS.from_formula('ar ~ const +   dividends + volume + market_cap+ surprise_percent + positive_label_1 + negative_label_1 + positive_label_0  + EntityEffects', data = model_run_final, check_rank=False, drop_absorbed=True)
H101  = m.fit(cov_type='clustered', cluster_entity=True )
print(H101)

m = lm.PanelOLS.from_formula('ar ~ const +   dividends + volume + market_cap+ surprise_percent + positive_label_1 + negative_label_1 + negative_label_0 + EntityEffects', data = model_run_final, check_rank=False, drop_absorbed=True)
H102  = m.fit(cov_type='clustered', cluster_entity=True )
print(H102)

m = lm.PanelOLS.from_formula('ar ~ const +   dividends  + market_cap+ volume + surprise_percent + positive_label_1 + negative_label_1 + positive_label_0 + negative_label_0 + EntityEffects', data = model_run_final, check_rank=False, drop_absorbed=True)
H103  = m.fit(cov_type='clustered', cluster_entity=True )
print(H103)



m = lm.PanelOLS.from_formula('ar ~ const +   dividends + volume + market_cap+ surprise_percent + rel_pos_label_1 + rel_neg_label_1  + EntityEffects', data = model_run_final, check_rank=False, drop_absorbed=True)
H104  = m.fit(cov_type='clustered', cluster_entity=True )
print(H104)


m = lm.PanelOLS.from_formula('ar ~ const +   dividends + volume + market_cap+ surprise_percent+ rel_pos_label_1 + rel_neg_label_1 + rel_pos_label_0 + EntityEffects', data = model_run_final, check_rank=False, drop_absorbed=True)
H105  = m.fit(cov_type='clustered', cluster_entity=True )
print(H105)


m = lm.PanelOLS.from_formula('ar ~ const +   dividends + volume + market_cap+ surprise_percent+ rel_pos_label_1 + rel_neg_label_1 + rel_neg_label_0 + EntityEffects', data = model_run_final, check_rank=False, drop_absorbed=True)
H106  = m.fit(cov_type='clustered', cluster_entity=True )
print(H106)

m = lm.PanelOLS.from_formula('ar ~ const +   dividends + volume + market_cap+ surprise_percent + rel_pos_label_1 + rel_neg_label_1 + rel_pos_label_0 + rel_neg_label_0 + EntityEffects', data = model_run_final, check_rank=False, drop_absorbed=True)
H107  = m.fit(cov_type='clustered', cluster_entity=True )
print(H107)



report = Stargazer([H100, H101, H102, H103, H104, H105, H106, H107])
##report.render_html()
with open("Report.html", "w") as file:
    file.write(report.render_html())

"""positive_label_1_uncert
negative_label_1_uncert
positive_label_0_uncert
negative_label_0_uncert
rel_pos_label_1_uncert +rel_neg_label_1_uncert
rel_pos_label_0_uncert + rel_neg_label_0_uncert
"""

model_run_final

formula = 'ar ~ const +   dividends  + market_cap+ volume + surprise_percent + positive_label_1 + negative_label_1 + positive_label_0 + negative_label_0 + EntityEffects'
m = lm.PanelOLS.from_formula(formula, data = model_run_final, check_rank=False, drop_absorbed=True)
H200  = m.fit(cov_type='clustered', cluster_entity=True )


formula = 'ar ~ const +   dividends  + market_cap+ volume + surprise_percent + positive_label_1 + negative_label_1 + positive_label_0 + negative_label_0 + positive_label_1_uncert + negative_label_1_uncert + EntityEffects'
m = lm.PanelOLS.from_formula(formula, data = model_run_final, check_rank=False, drop_absorbed=True)
H201  = m.fit(cov_type='clustered', cluster_entity=True )


formula = 'ar ~ const +   dividends  + market_cap+ volume + surprise_percent + positive_label_1 + negative_label_1 + positive_label_0 + negative_label_0 +positive_label_1_uncert + negative_label_1_uncert+ positive_label_0_uncert + negative_label_0_uncert + EntityEffects'
m = lm.PanelOLS.from_formula(formula, data = model_run_final, check_rank=False, drop_absorbed=True)
H202  = m.fit(cov_type='clustered', cluster_entity=True )


"""formula = 'ar ~ const +   dividends  + market_cap+ volume + surprise_percent + positive_label_1 + negative_label_1 + positive_label_0 + negative_label_0 + EntityEffects'
m = lm.PanelOLS.from_formula(formula, data = model_run_final, check_rank=False, drop_absorbed=True)
H103  = m.fit(cov_type='clustered', cluster_entity=True )"""




formula = 'ar ~ const +   dividends + volume + market_cap+ surprise_percent + rel_pos_label_1 + rel_neg_label_1 + rel_pos_label_0 + rel_neg_label_0 + EntityEffects'
m = lm.PanelOLS.from_formula(formula, data = model_run_final, check_rank=False, drop_absorbed=True)
H204  = m.fit(cov_type='clustered', cluster_entity=True )



formula = 'ar ~ const +   dividends + volume + market_cap+ surprise_percent + rel_pos_label_1 + rel_neg_label_1 + rel_pos_label_0 + rel_neg_label_0 + rel_pos_label_1_uncert +rel_neg_label_1_uncert + EntityEffects'
m = lm.PanelOLS.from_formula(formula, data = model_run_final, check_rank=False, drop_absorbed=True)
H205  = m.fit(cov_type='clustered', cluster_entity=True )



formula = 'ar ~ const +   dividends + volume + market_cap+ surprise_percent + rel_pos_label_1 + rel_neg_label_1 + rel_pos_label_0 + rel_neg_label_0 + rel_pos_label_1_uncert +rel_neg_label_1_uncert+ rel_pos_label_0_uncert + rel_neg_label_0_uncert + EntityEffects'
m = lm.PanelOLS.from_formula(formula, data = model_run_final, check_rank=False, drop_absorbed=True)
H206  = m.fit(cov_type='clustered', cluster_entity=True )


'''formula = 'ar ~ const +   dividends + volume + market_cap+ surprise_percent + rel_pos_label_1 + rel_neg_label_1 + rel_pos_label_0 + rel_neg_label_0 + EntityEffects'
m = lm.PanelOLS.from_formula(formula, data = model_run_final, check_rank=False, drop_absorbed=True)
H107  = m.fit(cov_type='clustered', cluster_entity=True )'''




report = Stargazer([H200, H201, H202, H204, H205, H206])
##report.render_html()
with open("Report2.html", "w") as file:
    file.write(report.render_html())

"""positive_label_1_illiq +negative_label_1_illiq
positive_label_0_illiq + negative_label_0_illiq
rel_pos_label_1_illiq + rel_neg_label_1_illiq
rel_pos_label_0_illiq + rel_neg_label_0_illiq
"""

formula = 'ar ~ const +   dividends  + market_cap+ volume + surprise_percent + positive_label_1 + negative_label_1 + positive_label_0 + negative_label_0 + EntityEffects'
m = lm.PanelOLS.from_formula(formula, data = model_run_final, check_rank=False, drop_absorbed=True)
H200  = m.fit(cov_type='clustered', cluster_entity=True )


formula = 'ar ~ const +   dividends  + market_cap+ volume + surprise_percent + positive_label_1 + negative_label_1 + positive_label_0 + negative_label_0 + positive_label_1_illiq +negative_label_1_illiq + EntityEffects'
m = lm.PanelOLS.from_formula(formula, data = model_run_final, check_rank=False, drop_absorbed=True)
H201  = m.fit(cov_type='clustered', cluster_entity=True )


formula = 'ar ~ const +   dividends  + market_cap+ volume + surprise_percent + positive_label_1 + negative_label_1 + positive_label_0 + negative_label_0 + positive_label_1_illiq +negative_label_1_illiq + positive_label_0_illiq + negative_label_0_illiq + EntityEffects'
m = lm.PanelOLS.from_formula(formula, data = model_run_final, check_rank=False, drop_absorbed=True)
H202  = m.fit(cov_type='clustered', cluster_entity=True )


"""formula = 'ar ~ const +   dividends  + market_cap+ volume + surprise_percent + positive_label_1 + negative_label_1 + positive_label_0 + negative_label_0 + EntityEffects'
m = lm.PanelOLS.from_formula(formula, data = model_run_final, check_rank=False, drop_absorbed=True)
H103  = m.fit(cov_type='clustered', cluster_entity=True )"""




formula = 'ar ~ const +   dividends + volume + market_cap+ surprise_percent + rel_pos_label_1 + rel_neg_label_1 + rel_pos_label_0 + rel_neg_label_0 + EntityEffects'
m = lm.PanelOLS.from_formula(formula, data = model_run_final, check_rank=False, drop_absorbed=True)
H204  = m.fit(cov_type='clustered', cluster_entity=True )



formula = 'ar ~ const +   dividends + volume + market_cap+ surprise_percent + rel_pos_label_1 + rel_neg_label_1 + rel_pos_label_0 + rel_neg_label_0 + rel_pos_label_1_illiq + rel_neg_label_1_illiq + EntityEffects'
m = lm.PanelOLS.from_formula(formula, data = model_run_final, check_rank=False, drop_absorbed=True)
H205  = m.fit(cov_type='clustered', cluster_entity=True )



formula = 'ar ~ const +   dividends + volume + market_cap+ surprise_percent + rel_pos_label_1 + rel_neg_label_1 + rel_pos_label_0 + rel_neg_label_0 + rel_pos_label_1_illiq + rel_neg_label_1_illiq + rel_pos_label_0_illiq + rel_neg_label_0_illiq + EntityEffects'
m = lm.PanelOLS.from_formula(formula, data = model_run_final, check_rank=False, drop_absorbed=True)
H206  = m.fit(cov_type='clustered', cluster_entity=True )


'''formula = 'ar ~ const +   dividends + volume + market_cap+ surprise_percent + rel_pos_label_1 + rel_neg_label_1 + rel_pos_label_0 + rel_neg_label_0 + EntityEffects'
m = lm.PanelOLS.from_formula(formula, data = model_run_final, check_rank=False, drop_absorbed=True)
H107  = m.fit(cov_type='clustered', cluster_entity=True )'''




report = Stargazer([H200, H201, H202, H204, H205, H206])
##report.render_html()
with open("Report3.html", "w") as file:
    file.write(report.render_html())

for model

m = lm.PanelOLS.from_formula('AR ~ positive*Label_1 +negative*Label_1 + positive*Label_0 + negative*Label_0 + Surprise(%) + Dividends + Market_Cap + Volume +const + EntityEffects', data = model_run[0], check_rank=False, drop_absorbed=True)
H1  = m.fit(cov_type='clustered', cluster_entity=True )
print(H1)

endog = model_run[0]['AR']

exogen = model_run[0][['positive*Label_1','negative*Label_1',
                       'positive*Label_0','negative*Label_0',
                       'Surprise(%)', 'Dividends', 'Market_Cap', 'Volume',  'const']]
m = lm.PanelOLS(endog,exogen, entity_effects=True, check_rank=False, drop_absorbed=True)
H1  = m.fit(cov_type='clustered', cluster_entity=True, )

print(H1)

exogen = model_run[0][['positive*Label_1','negative*Label_1',
                       'positive*Label_0','negative*Label_0',
                       'positive*Label_1_Uncert',
                       'negative*Label_1_Uncert', 'positive*Label_0_Uncert',
                       'negative*Label_0_Uncert', 'Dividends', 'Market_Cap', 'Volume',  'const']]
m = lm.PanelOLS(endog,exogen, entity_effects=True)
H2  = m.fit(cov_type='clustered', cluster_entity=True)

print(H2)


exogen = model_run[0][['positive*Label_1','negative*Label_1',
                       'positive*Label_0','negative*Label_0',
                       'Surprise(%)', 'positive*Label_1_Illiq',
                       'negative*Label_1_Illiq', 'positive*Label_0_Illiq',
                       'negative*Label_0_Illiq',  'Market_Cap', 'Volume', 'const']]
m = lm.PanelOLS(endog,exogen, entity_effects=True)
H3  = m.fit(cov_type='clustered', cluster_entity=True)

print(H3)

report = Stargazer([H1, H2, H3])
##report.render_html()
with open("Report.html", "w") as file:
    file.write(report.render_html())

!pip install stargazer
from stargazer.stargazer import Stargazer

endog = model_run[0]['AR']

exogen = model_run[0][['positive*Label_1','negative*Label_1',
                       'positive*Label_0','negative*Label_0',
                       'Surprise(%)', 'Dividends', 'Market_Cap', 'Volume', 'const']]
m = lm.PanelOLS(endog,exogen, entity_effects=True)
H1  = m.fit(cov_type='clustered', cluster_entity=True)

print(H1)

exogen = model_run[0][['positive*Label_1','negative*Label_1',
                       'positive*Label_0','negative*Label_0',
                       'positive*Label_1_Uncert',
                       'negative*Label_1_Uncert', 'positive*Label_0_Uncert',
                       'negative*Label_0_Uncert', 'Surprise(%)', 'Dividends', 'Market_Cap', 'Volume', 'const']]
m = lm.PanelOLS(endog,exogen, entity_effects=True)
H2  = m.fit(cov_type='clustered', cluster_entity=True)

print(H2)


exogen = model_run[0][['positive*Label_1','negative*Label_1',
                       'positive*Label_0','negative*Label_0',
                       'Surprise(%)', 'positive*Label_1_Illiq',
                       'negative*Label_1_Illiq', 'positive*Label_0_Illiq',
                       'negative*Label_0_Illiq', 'Dividends', 'Market_Cap', 'Volume', 'const']]
m = lm.PanelOLS(endog,exogen, entity_effects=True)
H3  = m.fit(cov_type='clustered', cluster_entity=True)

print(H3)

model_run[0].to_csv( 'H1.csv')

endog = model_run[0]['AR']

exogen = model_run[0][['Rel_pos*Label_1','Rel_neg*Label_1',
                       'Rel_pos*Label_0','Rel_neg*Label_0',
                       'Surprise(%)', 'const']]
m = lm.PanelOLS(endog,exogen, entity_effects=True)
H1  = m.fit(cov_type='clustered', cluster_entity=True)

print(H1)

exogen = model_run[0][['Rel_pos*Label_1','Rel_neg*Label_1',
                       'Rel_pos*Label_0','Rel_neg*Label_0', 'Rel_pos*Label_1_Uncert',
                       'Rel_neg*Label_1_Uncert', 'Rel_pos*Label_0_Uncert',
                       'Rel_neg*Label_0_Uncert', 'Surprise(%)', 'const']]
m = lm.PanelOLS(endog,exogen, entity_effects=True)
H2  = m.fit(cov_type='clustered', cluster_entity=True)

print(H2)


exogen = model_run[0][['Rel_pos*Label_1','Rel_neg*Label_1',
                       'Rel_pos*Label_0','Rel_neg*Label_0',
                       'Surprise(%)', 'Rel_pos*Label_1_Illiq',
                       'Rel_neg*Label_1_Illiq', 'Rel_pos*Label_0_Illiq',
                       'Rel_neg*Label_0_Illiq', 'const']]
m = lm.PanelOLS(endog,exogen, entity_effects=True)
H3  = m.fit(cov_type='clustered', cluster_entity=True)

print(H3)

def test_Sentiment(df, level = ["_20_", "_40_", "_60_", "_120_"], relevance = True):

  endog = df['AR']
  df['const'] = 1


  keyword = level
  control_var = ['Surprise(%)', 'const']

  if relevance == True:

    H1_list = ['RPL1_20_index',
              'RPL0_20_index', 'RPL1_40_index',
              'RPL0_40_index', 'RPL1_60_index',
              'RPL0_60_index',  'RPL1_120_index',
              'RPL0_120_index']

    H2_list = ['RPL1_20_index_Uncert',
              'RPL0_20_index_Uncert', 'RPL1_40_index_Uncert',
              'RPL0_40_index_Uncert', 'RPL1_60_index_Uncert',
              'RPL0_60_index_Uncert',  'RPL1_120_index_Uncert',
              'RPL0_120_index_Uncert']

    H3_list = ['RPL1_20_index_Illiq',
              'RPL0_20_index_Illiq', 'RPL1_40_index_Illiq',
              'RPL0_40_index_Illiq', 'RPL1_60_index_Illiq',
              'RPL0_60_index_Illiq',  'RPL1_120_index_Illiq',
              'RPL0_120_index_Illiq']

  else:
    H1_list = ['PL1_20_index',
              'PL0_20_index', 'PL1_40_index',
              'PL0_40_index', 'PL1_60_index',
              'PL0_60_index',  'PL1_120_index',
              'PL0_120_index']

    H2_list = ['PL1_20_index_Uncert',
              'PL0_20_index_Uncert', 'PL1_40_index_Uncert',
              'PL0_40_index_Uncert', 'PL1_60_index_Uncert',
              'PL0_60_index_Uncert',  'PL1_120_index_Uncert',
              'PL0_120_index_Uncert']

    H3_list = ['PL1_20_index_Illiq',
              'PL0_20_index_Illiq', 'PL1_40_index_Illiq',
              'PL0_40_index_Illiq', 'PL1_60_index_Illiq',
              'PL0_60_index_Illiq',  'PL1_120_index_Illiq',
              'PL0_120_index_Illiq']




    H1_list_level = [word for word in H1_list if all(val in word for val in keyword.split(' '))]
    H2_list_level = [word for word in H2_list if all(val in word for val in keyword.split(' '))]
    H3_list_level = [word for word in H3_list if all(val in word for val in keyword.split(' '))]


    H2_list_level = H1_list_level + H2_list_level + control_var
    H3_list_level = H1_list_level + H3_list_level + control_var
    H1_list_level = H1_list_level + control_var

    print(H1_list_level)
    print(H2_list_level)
    print(H3_list_level)




  ## H1 hypothesis
  exogen = df[H1_list_level]
  m = lm.PanelOLS(endog, exogen, entity_effects=True)
  H1  = m.fit(cov_type='clustered', cluster_entity=True)
  ##H1  = m.fit(cov_type='robust')
  print(H1)

  ## H2 hypothesis
  exogen = df[H2_list_level]
  m = lm.PanelOLS(endog, exogen, entity_effects=True)
  H2  = m.fit(cov_type='clustered', cluster_entity=True)
  ##H2  = m.fit(cov_type='robust')
  print(H2)

  ## H3 hypothesis
  exogen = df[H3_list_level]
  m = lm.PanelOLS(endog, exogen, entity_effects=True)
  H3  = m.fit(cov_type='clustered', cluster_entity=True)
  ##H3  = m.fit(cov_type='robust')
  print(H3)



  return H1, H2, H3







H1, H2, H3 =test_Sentiment(model_run[0], "_120_", False)

News_df[1]

News_df[1][['Title', 'FakeBERT', 'FinBERT', 'Rel_neg', 'Rel_neut', 'Rel_pos']]



endog = model_run[0]['AR']
exogen = model_run[0][['RPL1_20_index',
 'RPL0_20_index', 'RPL1_40_index',
 'RPL0_40_index', 'RPL1_60_index',
 'RPL0_60_index',  'RPL1_120_index',
 'RPL0_120_index', 'Surprise(%)', 'const']]
m = lm.PanelOLS(endog,exogen, entity_effects=True)
H4  = m.fit(cov_type='clustered', cluster_entity=True)

H4

import statsmodels.api as sm

test = model_run[0].reset_index()
test['const'] = 1
y = test["AR"]

##H1
X = test[['Rel_pos*Label_1', 'Rel_neg*Label_1','Rel_pos*Label_0', 'Rel_neg*Label_0', 'Surprise(%)', 'const']]

##H2
X = test[['Rel_pos*Label_1', 'Rel_neg*Label_1','Rel_pos*Label_0', 'Rel_neg*Label_0', 'Surprise(%)', 'Volume', 'Dividends', 'Stock Splits', 'const']]


##
model = sm.OLS(y, X)
results = model.fit()

print(results.summary())
Rel_neg*Label_1



import statsmodels.api as sm

test = model_run[0].reset_index()
y = test["AR"]
X = test.drop(["symbol",	"Event_only_index"	, "AR"], axis=1)
model = sm.OLS(y, X)
results = model.fit()

print(results.summary())

import statsmodels.api as sm

model = sm.GLM(y, X)
results = model.fit_regularized()
##print(results.summary())

results

test = model_run[0].reset_index()
y = test["AR"]
X = test.drop(["symbol",	"Event_only_index"	, "AR"], axis=1)

clf.fit(X, y)

df_stock

News_df[0]

df_stock = pd.merge(df_stock, News_df.reset_index(), left_on=['symbol', 'Date'],
                       right_on=['Company', 'Date'], how="inner")

## Market uncertanty index
for date in df_stock.Date.unique():
  Set = df_stock[df_stock.Date == date]
  Positive_sum = Set['positive'].sum()
  Negative_sum = Set['negative'].sum()
  try:
    Uncert = max(0, 1- Negative_sum / (Positive_sum + Negative_sum))
  except:
    Uncert = 0
  df_stock.loc[df_stock.Date == date, 'Uncert']  = Uncert

## Event window
df_stock["Event_only_index"] =np.nan
df_stock.loc[(df_stock["Event"]==1) & (df_stock["Event_ID"].notna()), "Event_only_index"] = df_stock["Event_ID"]


## Amihud computation

df_stock["Illiquidity"] = abs(df_stock["Returns"])/(df_stock["High"] * df_stock["High"]*df_stock["Volume"]/2)

grouped = df_stock.drop("Date", axis=1).groupby('symbol')

results = {'symbol':[], 'illiquidity':[]}
for item, grp in grouped:
    subset_mean = grp.tail(2).sum()[0]
    results['symbol'].append(item)
    results['illiquidity'].append(subset_mean)

res_df = pd.DataFrame(results)
Quant = res_df["illiquidity"].quantile(0.75)
res_df['Illiq'] = res_df['illiquidity'].ge(Quant).astype(int)
df_stock = df_stock.merge(res_df, on='symbol', how='left')

## Amihud computation

df_stock["Illiquidity"] = abs(df_stock["Returns"])/(df_stock["High"] * df_stock["High"]*df_stock["Volume"]/2)

grouped = df_stock.drop("Date", axis=1).groupby('symbol')

results = {'symbol':[], 'illiquidity':[]}
for item, grp in grouped:
    subset_mean = grp.tail(2).sum()[0]
    results['symbol'].append(item)
    results['illiquidity'].append(subset_mean)

res_df = pd.DataFrame(results)
Quant = res_df["illiquidity"].quantile(0.75)
res_df['Illiq'] = res_df['illiquidity'].ge(Quant).astype(int)
test = df_stock.merge(res_df, on='symbol', how='left')

df_stock = df_stock.groupby(['symbol', 'Date']).sum()

New_cols = ["PL1_", "NL1_", "PL0_", "NL0_", "RPL1_","RNL1_", "RPL0_", "RNL0_"]
old_cols = ['positive*Label_1', 'negative*Label_1', 'positive*Label_0', 'negative*Label_0', 'Rel_pos*Label_1', 'Rel_neg*Label_1',
       'Rel_pos*Label_0', 'Rel_neg*Label_0']
for lag in [20, 40, 60, 120]:
  names = [ i + str(lag) for i in New_cols]
  print(names)
  df_stock[names] = df_stock[old_cols].rolling(window=lag).sum()

df_stock.reset_index(inplace=True)

test = df_stock[['AR', 'EPS Estimate', 'Reported EPS',
       'Surprise(%)', 'symbol', 'Event', 'Event_ID', 'positive*Label_1',
       'negative*Label_1', 'positive*Label_0', 'negative*Label_0', 'Uncert',
       'Event_only_index', 'Rel_pos*Label_1', 'Rel_neg*Label_1',
       'Rel_pos*Label_0', 'Rel_neg*Label_0', "Illiq",'PL1_20', 'NL1_20', 'PL0_20',
       'NL0_20', 'RPL1_20', 'RNL1_20', 'RPL0_20', 'RNL0_20', 'PL1_40',
       'NL1_40', 'PL0_40', 'NL0_40', 'RPL1_40', 'RNL1_40', 'RPL0_40',
       'RNL0_40', 'PL1_60', 'NL1_60', 'PL0_60', 'NL0_60', 'RPL1_60', 'RNL1_60',
       'RPL0_60', 'RNL0_60', 'PL1_120', 'NL1_120', 'PL0_120', 'NL0_120',
       'RPL1_120', 'RNL1_120', 'RPL0_120', 'RNL0_120'
                 ]]

test2 = test.drop(test[test['Event_only_index'] == 0.0].index)
model_run = test2.groupby(['symbol', 'Event_only_index']).sum()

def new_dummy(df, key_col, col_to_apply):
  for i in col_to_apply:
    name = i +"_" + key_col
    print(name)
    df[name] = df[key_col]* df[i]
  return df


def new_index(df, Positive, all_cols, scale=10):
  for i in Positive:
    name = i +"_" + "index"
    print(name)

    Index_of_Negative = all_cols.index(i) +1
    neg_col = all_cols[Index_of_Negative]
    df[name] = (df[i] - df[neg_col])/ (df[i] + df[neg_col])/10
  return df

model_run.columns

Lag_list = ['PL1_20', 'NL1_20',
       'PL0_20', 'NL0_20', 'RPL1_20', 'RNL1_20', 'RPL0_20', 'RNL0_20',
       'PL1_40', 'NL1_40', 'PL0_40', 'NL0_40', 'RPL1_40', 'RNL1_40', 'RPL0_40',
       'RNL0_40', 'PL1_60', 'NL1_60', 'PL0_60', 'NL0_60', 'RPL1_60', 'RNL1_60',
       'RPL0_60', 'RNL0_60', 'PL1_120', 'NL1_120', 'PL0_120', 'NL0_120',
       'RPL1_120', 'RNL1_120', 'RPL0_120', 'RNL0_120']


Positive_values = ['PL1_20', 'PL0_20',  'RPL1_20',  'RPL0_20',
       'PL1_40', 'PL0_40',  'RPL1_40', 'RPL0_40', 'PL1_60', 'PL0_60', 'RPL1_60',
       'RPL0_60', 'PL1_120', 'PL0_120', 'RPL1_120',  'RPL0_120']
Negative_values = list(set(Lag_list) - set(Positive_values))
model_run = new_index(model_run, Positive_values, Lag_list)


model_run = new_dummy(model_run, "Uncert", Lag_list)
model_run = new_dummy(model_run, "Illiq", Lag_list)

print(model_run.columns.values)

print(model_run.columns,  sep='\n')

['PL1_20_index', 'PL0_20_index', 'RPL1_20_index',
 'RPL0_20_index', 'PL1_40_index', 'PL0_40_index', 'RPL1_40_index',
 'RPL0_40_index', 'PL1_60_index', 'PL0_60_index', 'RPL1_60_index',
 'RPL0_60_index', 'PL1_120_index', 'PL0_120_index', 'RPL1_120_index',
 'RPL0_120_index']

endog = model_run['AR']
exogen = model_run[['RPL1_20_index',
 'RPL0_20_index', 'RPL1_40_index',
 'RPL0_40_index', 'RPL1_60_index',
 'RPL0_60_index',  'RPL1_120_index',
 'RPL0_120_index']]
m = lm.PanelOLS(endog,exogen, entity_effects=True)
H1  = m.fit(cov_type='clustered', cluster_entity=True)

H1

endog = model_run['AR']
exogen = model_run[['PL1_20', 'NL1_20', 'PL0_20',
 'NL0_20', 'RPL1_20', 'RNL1_20', 'RPL0_20', 'RNL0_20', 'PL1_40', 'NL1_40',
 'PL0_40', 'NL0_40', 'RPL1_40', 'RNL1_40', 'RPL0_40', 'RNL0_40', 'PL1_60',
 'NL1_60', 'PL0_60', 'NL0_60', 'RPL1_60', 'RNL1_60', 'RPL0_60', 'RNL0_60',
 'PL1_120', 'NL1_120', 'PL0_120', 'NL0_120', 'RPL1_120', 'RNL1_120', 'RPL0_120',
 'RNL0_120']]
m = lm.PanelOLS(endog,exogen, entity_effects=True)
H1  = m.fit(cov_type='clustered', cluster_entity=True)

H1

New_cols = ["N_", "n_", "P_", "L0_", "L1_", "RN_", "Rn_", "RP_" ]
old_cols = ['negative', 'neutral', 'positive', 'LABEL_0', 'LABEL_1', 'Rel_neg',
       'Rel_neut', 'Rel_pos']
for lag in [20, 40, 60, 120]:
  names = [ i + str(lag) for i in New_cols]
  print(names)
  News_df[names] = News_df[old_cols].rolling(window=lag).sum()
new_dummy

model_run['Uncert'] = model_run['Uncert']/10
model_run['Illiq'] = model_run['Illiq']/10

model_run['Uncert*positive*Label_1'] = model_run['positive*Label_1']* model_run['Uncert']
model_run['Uncert*negative*Label_1'] = model_run['negative*Label_1']* model_run['Uncert']
model_run['Uncert*positive*Label_0'] = model_run['positive*Label_0']* model_run['Uncert']
model_run['Uncert*negative*Label_0'] = model_run['negative*Label_0']* model_run['Uncert']

model_run['Illiq*positive*Label_1'] = model_run['positive*Label_1']* model_run['Illiq']
model_run['Illiq*negative*Label_1'] = model_run['negative*Label_1']* model_run['Illiq']
model_run['Illiq*positive*Label_0'] = model_run['positive*Label_0']* model_run['Illiq']
model_run['Illiq*negative*Label_0'] = model_run['negative*Label_0']* model_run['Illiq']




model_run['Uncert*Rel_pos*Label_1'] = model_run['Rel_pos*Label_1']* model_run['Uncert']
model_run['Uncert*Rel_neg*Label_1'] = model_run['Rel_neg*Label_1']* model_run['Uncert']
model_run['Uncert*Rel_pos*Label_0'] = model_run['Rel_pos*Label_0']* model_run['Uncert']
model_run['Uncert*Rel_neg*Label_0'] = model_run['Rel_neg*Label_0']* model_run['Uncert']

model_run['Illiq*Rel_pos*Label_1'] = model_run['Rel_pos*Label_1']* model_run['Illiq']
model_run['Illiq*Rel_neg*Label_1'] = model_run['Rel_neg*Label_1']* model_run['Illiq']
model_run['Illiq*Rel_pos*Label_0'] = model_run['Rel_pos*Label_0']* model_run['Illiq']
model_run['Illiq*Rel_neg*Label_0'] = model_run['Rel_neg*Label_0']* model_run['Illiq']




model_run["True_index"] = (model_run["positive*Label_1"] -model_run["negative*Label_1"])/(model_run["positive*Label_1"] + model_run["negative*Label_1"])
model_run["Fake_index"] = (model_run["positive*Label_0"] -model_run["negative*Label_0"])/(model_run["positive*Label_0"] + model_run["negative*Label_0"])
model_run['Illiq*True_index'] = model_run['True_index']* model_run['Illiq']
model_run['Illiq*Fake_index'] = model_run['Fake_index']* model_run['Illiq']
model_run['Uncert*True_index'] = model_run['True_index']* model_run['Uncert']
model_run['Uncert*Fake_index'] = model_run['Fake_index']* model_run['Uncert']



model_run["Rel_True_index"] = (model_run["Rel_pos*Label_1"] -model_run["Rel_neg*Label_1"])/(model_run["Rel_pos*Label_1"] + model_run["Rel_neg*Label_1"])
model_run["Rel_Fake_index"] = (model_run["Rel_pos*Label_0"] -model_run["Rel_neg*Label_0"])/(model_run["Rel_pos*Label_0"] + model_run["Rel_neg*Label_0"])
model_run['Illiq*Rel_True_index'] = model_run['Rel_True_index']* model_run['Illiq']
model_run['Illiq*Rel_Fake_index'] = model_run['Rel_Fake_index']* model_run['Illiq']
model_run['Uncert*Rel_True_index'] = model_run['Rel_True_index']* model_run['Uncert']
model_run['Uncert*Rel_Fake_index'] = model_run['Rel_Fake_index']* model_run['Uncert']

model_run.reset_index()

model_run['Uncert']

model_run.describe()

model_run[['True_index', "Fake_index", "Rel_True_index" , "Rel_Fake_index"]].describe()

News_df[["Title", 'negative',	'neutral',	'positive', 'Rel_neg',	'Rel_neut', 'Rel_pos']]

model_run.columns

!pip install linearmodels -q
import linearmodels as lm

endog = model_run['AR']
exogen = model_run[['positive*Label_1', 'negative*Label_1', 'positive*Label_0',
       'negative*Label_0', 'Surprise(%)']]
m = lm.PanelOLS(endog,exogen, entity_effects=True)
H0  = m.fit(cov_type='clustered', cluster_entity=True)

H0

!pip install linearmodels -q
import linearmodels as lm

endog = model_run['AR']
exogen = model_run[['positive*Label_1', 'negative*Label_1', 'positive*Label_0',
       'negative*Label_0', 'Illiq*positive*Label_1', 'Illiq*negative*Label_1',
       'Illiq*positive*Label_0', 'Illiq*negative*Label_0', 'Surprise(%)']]
m = lm.PanelOLS(endog,exogen, entity_effects=True)
H0  = m.fit(cov_type='clustered', cluster_entity=True)

H0

!pip install linearmodels -q
import linearmodels as lm

endog = model_run['AR']
exogen = model_run[['positive*Label_1', 'negative*Label_1', 'positive*Label_0',
       'negative*Label_0', 'Uncert*positive*Label_1', 'Uncert*negative*Label_1',
       'Uncert*positive*Label_0', 'Uncert*negative*Label_0', 'Surprise(%)']]
m = lm.PanelOLS(endog,exogen, entity_effects=True)
H4  = m.fit(cov_type='clustered', cluster_entity=True)

H4

!pip install linearmodels -q
import linearmodels as lm

endog = model_run['AR']
exogen = model_run[['True_index',
       'Fake_index', 'Illiq*True_index', 'Illiq*Fake_index', 'Surprise(%)']]
m = lm.PanelOLS(endog,exogen, entity_effects=True)
H4  = m.fit(cov_type='clustered', cluster_entity=True)

H4

model_run.columns

!pip install linearmodels -q
import linearmodels as lm

endog = model_run['AR']
exogen = model_run[['Rel_pos*Label_1', 'Rel_neg*Label_1',
       'Rel_pos*Label_0','Rel_neg*Label_0', 'Surprise(%)']]
m = lm.PanelOLS(endog,exogen, entity_effects=True)
H1  = m.fit(cov_type='clustered', cluster_entity=True)

H1

!pip install linearmodels -q
import linearmodels as lm

endog = model_run['AR']
exogen = model_run[['Rel_pos*Label_1', 'Rel_neg*Label_1',
       'Rel_pos*Label_0', 'Rel_neg*Label_0','Illiq*Rel_pos*Label_1', 'Illiq*Rel_neg*Label_1',
       'Illiq*Rel_pos*Label_0', 'Illiq*Rel_neg*Label_0', 'Surprise(%)']]
m = lm.PanelOLS(endog,exogen, entity_effects=True)
H2  = m.fit(cov_type='clustered', cluster_entity=True)

H2

model_run.columns

model_run

!pip install linearmodels -q
import linearmodels as lm

endog = model_run['AR']
exogen = model_run[['Rel_pos*Label_1', 'Rel_neg*Label_1',
       'Rel_pos*Label_0', 'Rel_neg*Label_0', 'Uncert*Rel_pos*Label_1',
       'Uncert*Rel_neg*Label_1', 'Uncert*Rel_pos*Label_0',
       'Uncert*Rel_neg*Label_0', 'Surprise(%)']]
m = lm.PanelOLS(endog,exogen, entity_effects=True)
H3  = m.fit(cov_type='clustered', cluster_entity=True)

H3

## Relevance

!pip install linearmodels -q
import linearmodels as lm

endog = model_run['AR']
exogen = model_run[['positive*Label_1', 'negative*Label_1', 'positive*Label_0',
       'negative*Label_0', 'Surprise(%)']]
m = lm.PanelOLS(endog,exogen, entity_effects=True)
H0  = m.fit(cov_type='clustered', cluster_entity=True)

H0



!pip install linearmodels -q
import linearmodels as lm

test[test["Event"]==1]



df_stock[df_stock['Event_only_index'].notna()]

df_full = df_stock[['Date','symbol', 'AR', 'positive*Label_1', 'negative*Label_1', 'positive*Label_0',
       'negative*Label_0']]

df_full.Date = pd.to_datetime(df_full.Date)
 df_full=df_full.set_index(['symbol', 'Date'])
##df_full= df_full.groupby(['symbol', 'Date']).sum()

!pip install linearmodels -q
import linearmodels as lm

df_full.columns

endog = df_full['AR']
exogen = df_full[['positive*Label_1', 'negative*Label_1', 'positive*Label_0',
       'negative*Label_0']]
m = lm.PanelOLS(endog,exogen, entity_effects=True)
H0  = m.fit(cov_type='clustered', cluster_entity=True)

print(H0)

df_stock

H0_df

H0_df = df_stock.copy()
H0_df["Date"]

H0_df = df_stock.copy()
H0_df["Date"] = pd.to_datetime(H0_df['Date'])
##H0_df['Date'] = H0_df['Date'].dt.date
H0_df.set_index(['symbol', "Date"], inplace =True)

endog = H0_df['AR']
exogen = H0_df[['positive*Label_1', 'negative*Label_1', 'positive*Label_0',
       'negative*Label_0']]
m = lm.PanelOLS(endog,exogen, entity_effects=True)
H0  = m.fit(cov_type='clustered', cluster_entity=True)

H0

H0_df.columns

H0_df = df_stock.copy()
H0_df["Date"] = pd.to_datetime(H0_df['Date'])
##H0_df['Date'] = H0_df['Date'].dt.date
H0_df.set_index(['symbol', "Date"], inplace =True)

endog = H0_df['AR']
exogen = H0_df[['Rel_pos*Label_1', 'Rel_neg*Label_1', 'Rel_pos*Label_0',
       'Rel_neg*Label_0']]
m = lm.PanelOLS(endog,exogen, entity_effects=True)
H0  = m.fit(cov_type='clustered', cluster_entity=True)

H0_df = df_stock.copy()
H0_df["Date"] = pd.to_datetime(H0_df['Date'])
##H0_df['Date'] = H0_df['Date'].dt.date
H0_df.set_index(['symbol', "Date"], inplace =True)

endog = H0_df['AR']
exogen = H0_df[['Rel_pos*Label_1', 'Rel_neg*Label_1', 'Rel_pos*Label_0',
       'Rel_neg*Label_0']]
m = lm.PanelOLS(endog,exogen, entity_effects=True)
H0  = m.fit(cov_type='clustered', cluster_entity=True)

H0

pd.read_excel("/content/Sources_class.xlsx", header=1).fillna(0)

News_df = ML.df_news.copy()
News_df =News_df.reset_index()

Date_col= pd.to_datetime(News_df['Date'], utc=True, errors='coerce')
News_df['Date'] = Date_col.dt.date



News_df['positive*Label_0'] = News_df['positive'] * News_df['LABEL_0']
News_df['negative*Label_0'] = News_df['negative'] * News_df['LABEL_0']
News_df['neutral*Label_0'] = News_df['neutral'] * News_df['LABEL_0']

News_df['positive*Label_1'] = News_df['positive'] * News_df['LABEL_1']
News_df['negative*Label_1'] = News_df['negative'] * News_df['LABEL_1']
News_df['neutral*Label_1'] = News_df['neutral'] * News_df['LABEL_1']

##Relevance adjustment
News_df['Rel_pos*Label_1'] = News_df['Rel_pos'] * News_df['LABEL_1']
News_df['Rel_neg*Label_1'] = News_df['Rel_neg'] * News_df['LABEL_1']
News_df['Rel_pos*Label_0'] = News_df['Rel_pos'] * News_df['LABEL_0']
News_df['Rel_neg*Label_0'] = News_df['Rel_neg'] * News_df['LABEL_0']

News_df['Rel_neut*Label_1'] = News_df['Rel_neut'] * News_df['LABEL_1']
News_df['Rel_neut*Label_0'] = News_df['Rel_neut'] * News_df['LABEL_0']

News_df = News_df[['Source', 'negative', 'neutral', 'positive', 'Rel_neg', 'Rel_neut', 'Rel_pos',
  'LABEL_0', 'LABEL_1']]

News_df.columns

sources_df = pd.read_excel("/content/Sources_class.xlsx", header=1).fillna(0)

test = News_df.merge(sources_df, on="Source", how='left')
test =test.drop("Source", axis=1)
test =test.drop("count", axis=1)

test[test["Relevant"]==1.0]

test2 = test[test["Relevant"]==1.0]

test['SomeCol']=1

test[test["Relevant"]==1.0].groupby("SomeCol").sum()

from io import StringIO
import warnings

def compare_df(x, fit_stats=['Estimator', 'R-squared', 'No. Observations']):
    with warnings.catch_warnings():
        warnings.simplefilter(action='ignore', category=FutureWarning)
        y = pd.read_csv(StringIO(compare(x, stars=True).summary.as_csv()), skiprows=1, skipfooter=1, engine='python')
    z = pd.DataFrame(
        data=y.iloc[:, 1:].values,
        index=y.iloc[:, 0].str.strip(),
        columns=pd.MultiIndex.from_arrays(
            arrays=[y.columns[1:], y.iloc[0][1:]],
            names=['Model', 'Dep. Var.']
        )
    )
    return pd.concat([z.iloc[11:], z.loc[fit_stats]])

from torch.utils.data import DataLoader

from transformers import AutoTokenizer
from datasets import Dataset

tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")



News_df = ML.df_news.copy()
News_df =News_df.groupby(['Company', 'Date']).agg({'Title':'sum'})
News_df =News_df.reset_index()
News_df.loc[:,'Date'] = pd.to_datetime(News_df.loc[:,'Date'])
News_df['Date'] = News_df['Date'].dt.date


stock = df_stock[['Date', 'AR_signif']].copy()
stock.columns = ['Date', 'labels']


stock = stock.merge(News_df).drop(['Date', 'Company'], axis=1)


stock = stock.dropna()
stock['labels'] = stock['labels'].astype(int)

stock[stock['labels'] == 0]

dataset = Dataset.from_pandas(stock.dropna().head(100000))


def tokenize_function(examples):
    return tokenizer(examples["Title"], padding="max_length", truncation=True)
tokenized_datasets = dataset.map(tokenize_function, batched=True)
del dataset



tokenized_datasets = tokenized_datasets.remove_columns(["Title"])
tokenized_datasets = tokenized_datasets.remove_columns(["__index_level_0__"])

tokenized_datasets.set_format("torch")

small_train_dataset = tokenized_datasets.shuffle(seed=117).select(range(5000))
small_eval_dataset = tokenized_datasets.shuffle(seed=42).select(range(1000))

train_size = int(0.8 * len(tokenized_datasets))
test_size = len(tokenized_datasets) - train_size
small_train_dataset, small_eval_dataset = torch.utils.data.random_split(tokenized_datasets, [train_size, test_size])

tokenized_datasets

"""###Trainer"""

ML = ML_build()
ML.news_df(News_dataset.head(5)) ##init with small size
ML.load_BERT()
ML.One_hot_encode(OHE=False)

##Import stock data and merge data on Ticker + Date
ML.import_stock_data()

import scipy.stats as sci

df_stock = ML.df_stock.copy()
##df_stock = df_stock.drop('Adj Close', axis=1)
df_stock.dropna()

def Beta_calculation(df,window, stock='Returns', market ='SP500_returns'):


  df['Beta'] = np.nan
  df['AR'] = np.nan
  df['AR_signif'] = np.nan




  risk_free = yf.Ticker('^IRX')
  risk_free = risk_free.history(period="5y")['Close'] *0.01
  risk_free =risk_free.reset_index()
  risk_free.loc[:,'Date'] = pd.to_datetime(risk_free.loc[:,'Date'])
  risk_free['Date'] = risk_free['Date'].dt.date


  for i in df['symbol'].unique():
    sliced = df[df['symbol'] == i].copy()
    covariance = sliced[stock].rolling(window).cov(sliced[market])
    variance = sliced[market].rolling(window).var()
    ##print(covariance, variance)

    beta = covariance/variance
    sliced['Beta']= beta
    ##df.loc[df['symbol'] == i, 'Beta'] = beta




    ##print(risk_free[risk_free['Date'].isin(sliced['Date'])])
    '''try:
      sliced['Risk_free']= risk_free[risk_free['Date'].isin(sliced['Date'])]['Close'].to_list()
    except:
      print(risk_free['Date'])
      sliced = sliced[sliced['Date'].isin(risk_free['Date'])]

      print(sliced['Date'])
      sliced['Risk_free']= risk_free[risk_free['Date'].isin(sliced['Date'])]['Close'].to_list()


    sliced['model_ret'] = sliced['Risk_free'] + sliced['Beta'] *( sliced[market] - sliced['Risk_free'])'''


    sliced['model_ret'] = 0 + sliced['Beta'] *( sliced[market])
    sliced['AR'] = sliced['Returns'] - sliced['model_ret']
    print(sliced['Returns'], sliced['model_ret'], sliced['Returns'] - sliced['model_ret'])
    sliced['AR'] = sliced['AR']*100

    '''est_window = 200
    sliced['Var'] = sliced['AR'].rolling(est_window, center=True).var()
    sliced['Statistic'] = sliced['AR']/ sliced['Var']
    crit_val = sci.norm.ppf(0.99)
    def test_crit(stat, crit):
      if stat >= crit:
        return 1
      elif stat <= -crit:
        return -1
      else:
        return 0
    sliced['AR_signif'] = sliced['Statistic'].apply(lambda x: test_crit(x, crit_val))'''
    crit_val = sci.norm.ppf(0.6)
    def test_crit(stat, crit):

      if stat >= crit:
        return 0
      elif stat <= -crit:
        return 1
      else:
        return 2
    sliced['AR_signif'] = sliced['AR'].apply(lambda x: test_crit(x, crit_val))




    df.update(sliced)
  return df
df_stock = Beta_calculation(df_stock, 120)

import scipy.stats as sci

df_stock = ML.df_stock.copy()
df_stock = df_stock.drop('Adj Close', axis=1)
df_stock.dropna()

def Beta_calculation(df,window, stock='Returns', market ='SP500_returns'):


  df['Beta'] = np.nan
  df['AR'] = np.nan
  df['AR_signif'] = np.nan




  risk_free = yf.Ticker('^IRX')
  risk_free = risk_free.history(period="5y")['Close'] *0.01
  risk_free =risk_free.reset_index()
  risk_free.loc[:,'Date'] = pd.to_datetime(risk_free.loc[:,'Date'])
  risk_free['Date'] = risk_free['Date'].dt.date


  for i in df['symbol'].unique():
    sliced = df[df['symbol'] == i].copy()

    print(sliced)
    covariance = sliced[stock].rolling(window).cov(sliced[market])
    variance = sliced[market].rolling(window).var()
    ##print(covariance, variance)

    beta = covariance/variance
    sliced['Beta']= beta

    print(sliced)
    ##df.loc[df['symbol'] == i, 'Beta'] = beta



    ##print(sliced)
    ##print(risk_free[risk_free['Date'].isin(sliced['Date'])])
    try:
      sliced['Risk_free']= risk_free[risk_free['Date'].isin(sliced['Date'])]['Close'].to_list()
    except:
      sliced = sliced[sliced['Date'].isin(risk_free['Date'])]
      sliced['Risk_free']= risk_free[risk_free['Date'].isin(sliced['Date'])]['Close'].to_list()


    sliced['model_ret'] = sliced['Risk_free'] + sliced['Beta'] *( sliced[market] - sliced['Risk_free'])
    sliced['AR'] = sliced['Returns'] - sliced['model_ret']
    sliced['AR'] = sliced['AR']*100

    '''est_window = 200
    sliced['Var'] = sliced['AR'].rolling(est_window, center=True).var()
    sliced['Statistic'] = sliced['AR']/ sliced['Var']
    crit_val = sci.norm.ppf(0.99)
    def test_crit(stat, crit):
      if stat >= crit:
        return 1
      elif stat <= -crit:
        return -1
      else:
        return 0
    sliced['AR_signif'] = sliced['Statistic'].apply(lambda x: test_crit(x, crit_val))'''
    crit_val = sci.norm.ppf(0.95)
    def test_crit(stat, crit):
      if stat >= crit:
        return "Positive_relevance"
      elif stat <= -crit:
        return "Negative_relevance"
      else:
        return "Not_relevant"
    sliced['AR_signif'] = sliced['AR'].apply(lambda x: test_crit(x, crit_val))




    df.update(sliced)
  return df
df_stock = Beta_calculation(df_stock, 30)

df_stock

df_stock[df_stock.AR_signif == 2][['Date', 'symbol', 'Returns', 'SP500_returns', 'Beta', 'AR', 'AR_signif']]

"""

distilbert-base-uncased
ProsusAI/finbert"""

from transformers import AutoModelForSequenceClassification
from transformers import TrainingArguments
import numpy as np
import evaluate
from transformers import AutoTokenizer
from datasets import Dataset

tokenizer = AutoTokenizer.from_pretrained("ProsusAI/finbert")

News_df = ML.df_news.copy()
News_df["Date"] =  pd.to_datetime(News_df["Date"]).dt.normalize()
News_df =News_df.groupby(['Company', 'Date']).agg({'Title':'sum'})
News_df =News_df.reset_index()
News_df.loc[:,'Date'] = pd.to_datetime(News_df.loc[:,'Date'])
News_df['Date'] = News_df['Date'].dt.date

stock = df_stock[['Date', 'symbol', 'AR_signif']].copy() ##see AR calc
stock.columns = ['Date', 'symbol', 'labels']
stock = pd.merge(stock, News_df, left_on=['symbol', 'Date'],right_on=['Company', 'Date'])
stock = stock.drop(['Date', 'symbol', 'Company'], axis=1)
stock = stock.dropna()
stock['labels'] = stock['labels'].astype(int)


g = stock.groupby('labels')
stock = g.apply(lambda x: x.sample(g.size().min())).reset_index(drop=True)
stock.columns = ['label', 'text']


dataset = Dataset.from_pandas(stock.dropna())
dataset = dataset.class_encode_column("label")
##dataset = Dataset.from_pandas(stock.dropna().head(2000000))


'''def tokenize_function(examples):
    return tokenizer(examples["text"], padding="max_length", truncation=True)
tokenized_datasets = dataset.map(tokenize_function, batched=True)
del dataset'''

def tokenize(batch):
    tokenized_batch = tokenizer(batch['text'], padding="max_length", truncation=True)
    ##tokenized_batch["labels"] = [str_to_int[label] for label in batch["labels"]]
    return tokenized_batch
tokenized_datasets = dataset.map(tokenize, batched=True)


'''tokenized_datasets = tokenized_datasets.remove_columns(["text"])
tokenized_datasets = tokenized_datasets.remove_columns(["__index_level_1__"])
tokenized_datasets = tokenized_datasets.remove_columns(["labels"])
'''
tokenized_datasets.set_format("torch")

##small_train_dataset = tokenized_datasets.shuffle(seed=117).select(range(5000))
##small_eval_dataset = tokenized_datasets.shuffle(seed=42).select(range(10000))

dataset_split = tokenized_datasets.train_test_split(test_size=0.1)
small_train_dataset = dataset_split['train']
small_eval_dataset = dataset_split['test']

model = AutoModelForSequenceClassification.from_pretrained("ProsusAI/finbert", num_labels=3)


device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = model.to(device)



training_args = TrainingArguments(output_dir="test_trainer")

metric = evaluate.load("accuracy")

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    return metric.compute(predictions=predictions, references=labels)


from transformers import TrainingArguments, Trainer

training_args = TrainingArguments(output_dir="test_trainer", evaluation_strategy="epoch", per_device_train_batch_size = 32,
                                  num_train_epochs=2)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=small_train_dataset,
    eval_dataset=small_eval_dataset,
    compute_metrics=compute_metrics,
)

stock

dataset

dataset

tokenized_datasets

tokenized_datasets['attention_mask']

stock[stock["labels"]]

stock = df_stock[['Date', 'symbol', 'AR_signif']].copy() ##see AR calc
stock.columns = ['Date', 'symbol', 'labels']
stock = pd.merge(stock, News_df, left_on=['symbol', 'Date'],right_on=['Company', 'Date'])
stock = stock.drop(['Date', 'symbol', 'Company'], axis=1)
stock = stock.dropna()
##stock['labels'] = stock['labels'].astype(int)


g = stock.groupby('labels')
stock = g.apply(lambda x: x.sample(g.size().min())).reset_index(drop=True)



stock

trainer.train()

trainer.save_model("/content/drive/MyDrive/Colab Notebooks/Data_paper/Relevance_model")

trainer.save_model("/content/drive/MyDrive/Colab Notebooks/Data_paper/BERT_trained_reserve")

test = AutoModelForSequenceClassification.from_pretrained("/content/drive/MyDrive/Colab Notebooks/Data_paper/Relevance_model (1)")

test

"""### PyTorch"""

small_train_dataset

from transformers import AutoModelForSequenceClassification
from torch.optim import AdamW
from transformers import get_scheduler
import torch

model = AutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased", num_labels=3)

optimizer = AdamW(model.parameters(), lr=5e-5)

num_epochs = 3
num_training_steps = num_epochs * len(train_dataloader)
lr_scheduler = get_scheduler(
    name="linear", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps
)


device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
model.to(device)

from tqdm.auto import tqdm

progress_bar = tqdm(range(num_training_steps))

model.train()
for epoch in range(num_epochs):
    for batch in train_dataloader:
        batch = {k: v.to(device) for k, v in batch.items()}
        outputs = model(**batch)
        loss = outputs.loss
        loss.backward()

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)

News_df

News_df = ML.df_news.copy()
News_df =News_df.reset_index()
News_df.loc[:,'Date'] = pd.to_datetime(News_df.loc[:,'Date'])
News_df['Date'] = News_df['Date'].dt.date
News_df =News_df.groupby(['Date']).sum()

News_df.columns = ['rows': News_df.columns]

"""### Graphs"""

News_df = ML.df_news.copy()
News_df =News_df.reset_index()
News_df.loc[:,'Date'] = pd.to_datetime(News_df.loc[:,'Date'])
News_df['Date'] = News_df['Date'].dt.date

import plotly.express as px

df.columns



News_df.drop(['index', 'Fin_BERT_score', 'FakeBERT_score', 'B-ORG',
              'positive*Label_1', 'negative*Label_1'], axis=1, inplace=True)

News_df.columns =['Negative sentiment',
                  'Neutral sentiment',
                  'Positive sentiment',
                  'Fake News',
                  'Legitimate News']

News_df.columns.name = 'News volume'

News_df

News_df1 = News_df[['Negative sentiment',
                  'Neutral sentiment',
                  'Positive sentiment']]

News_df2 = News_df[['Fake News',
                  'Legitimate News']]

News_df1

fig = px.line(News_df1, facet_col='News volume', facet_col_wrap=1)
fig.update_layout(
    xaxis_title="Date", yaxis_title="Daily Volume"
)

##fig.for_each_yaxis(lambda yaxis: yaxis.update(title="Daily Volume"))
##fig.update_yaxes(title="Daily Volume")

fig.for_each_yaxis(lambda y: y.update(title = ''))
# and:
fig.add_annotation(x=-0.1,y=0.5,
                   text="Daily Volume", textangle=-90,
                    xref="paper", yref="paper")
fig.show()

fig = px.line(News_df2, facet_col='News volume', facet_col_wrap=1)
fig.update_layout(
    xaxis_title="Date", yaxis_title="Daily Volume"
)

##fig.for_each_yaxis(lambda yaxis: yaxis.update(title="Daily Volume"))
##fig.update_yaxes(title="Daily Volume")

fig.for_each_yaxis(lambda y: y.update(title = ''))
# and:
fig.add_annotation(x=-0.1,y=0.5,
                   text="Daily Volume", textangle=-90,
                    xref="paper", yref="paper")
fig.show()

fig = px.line(News_df1, facet_col='News volume', facet_col_wrap=2)
fig.update_layout(
    xaxis_title="Date", yaxis_title="Daily Volume"
)

##fig.for_each_yaxis(lambda yaxis: yaxis.update(title="Daily Volume"))
##fig.update_yaxes(title="Daily Volume")

fig.for_each_yaxis(lambda y: y.update(title = ''))
# and:
fig.add_annotation(x=-0.1,y=0.5,
                   text="Daily Volume", textangle=-90,
                    xref="paper", yref="paper")
fig.show()

News_df.reset_index()

old_Cols = News_df.columns

import plotly.express as px
df = px.data.stocks()
fig = px.scatter(News_df, x=pd.to_datetime(News_df.index), y=old_Cols,
              title='News volume',  trendline="lowess", opacity=0.0)
fig.update_xaxes(
    dtick="M1",
    tickformat="%b n%Y")
fig.show()

"""'linear', 'hv', 'vh', 'hvh', 'vhv'"""

News_df[(News_df["Date"] >pd.to_datetime('2020-01-01'))]



"""#### GRaths 2

"""

ML.df_news

Export_df = ML.df_news.head(100000).copy()

Export_df.to_excel('export_df3.xlsx')

News_df = ML.df_news.copy()
Date_col= pd.to_datetime(News_df['Date'], utc=True, errors='coerce')
News_df['Date'] = Date_col.dt.date

News_df

sources_df = pd.read_excel("/content/Sources_class.xlsx", header=1).fillna(0)

News_df = News_df.merge(sources_df, on="Source", how='left')
##News_df =News_df.drop("Source", axis=1)
##News_df =News_df.drop("count", axis=1)
##News_df = News_df[(News_df['Relevant'] >0) | (News_df['Traditional'] >0)]

News_df['Company'].value_counts()

News_df[News_df['Company'] == "TXN"]

Curated_df =News_df[News_df['Company'] == 'TXN']
Curated_df2 = Curated_df.groupby(['Company','Date']).sum()
Curated_df2.drop(['Title', 'link', 'Ex.', 'FinBERT', 'Fin_BERT_score', 'FakeBERT',
                  'FakeBERT_score', 'Source', 'B-ORG', 'label', 'score'], axis=1, inplace=True)
Curated_df2.describe()

Curated_df =News_df[News_df['Company'] == 'UNH']
Curated_df2 = Curated_df.groupby(['Company','Date']).sum()
Curated_df2.drop(['Title', 'link', 'Ex.', 'FinBERT', 'Fin_BERT_score', 'FakeBERT',
                  'FakeBERT_score', 'Source', 'B-ORG', 'label', 'score'], axis=1, inplace=True)
Curated_df2.describe()

for i in ['QCOM','TXN', 'UNH']:

  print(i)

  Curated_df =News_df[News_df['Company'] == i]
  Curated_df2 = Curated_df.groupby(['Company','Date']).sum()
  Curated_df2.drop(['Title', 'link', 'Ex.', 'FinBERT', 'Fin_BERT_score', 'FakeBERT',
                    'FakeBERT_score', 'Source', 'B-ORG', 'label', 'score'], axis=1, inplace=True)
  print(Curated_df2.describe())





Curated_df2

Curated_df2.reset_index(inplace = True)

Curated_df2.describe()

Curated_df = News_df.copy()

def map_func(df):

  if df['Relevant'] > 0:
    return 1
  elif df['Traditional'] > 0:
    return 2
  else:
    return 0

Curated_df['Source_type'] = Curated_df.apply(lambda x: map_func(x), axis=1)

Curated_df.drop(['Title', 'link', 'Ex.', 'FinBERT', 'Fin_BERT_score', 'FakeBERT',
                    'FakeBERT_score', 'Source', 'B-ORG', 'label', 'score'], axis=1, inplace=True)

Curated_df.drop(['Date'], axis=1, inplace=True)

Curated_df.groupby(['Source_type']).sum()

Curated_df[Curated_df['Source_type'] == 2].describe()

Curated_df2 = Curated_df.groupby(['Company','Source_type']).sum()

"""## paper graphs"""

model_run_final['uncert']

["const", "dividends","market_cap","volume", "uncert", "surprise_percent",
 "positive_label_1","negative_label_1","positive_label_0","negative_label_0","negative_label_0",
 "positive_label_1_uncert","negative_label_1_uncert","positive_label_0_uncert","negative_label_0_uncert",
 "positive_label_1_illiq","negative_label_1_illiq","positive_label_0_illiq","negative_label_0_illiq",
 "rel_pos_label_1","rel_neg_label_1","rel_pos_label_0","rel_neg_label_0",
 "rel_pos_label_1_uncert","rel_neg_label_1_uncert","rel_pos_label_0_uncert","rel_neg_label_0_uncert",
 "rel_pos_label_1_illiq","rel_neg_label_1_illiq","rel_pos_label_0_illiq","rel_neg_label_0_illiq"]
rel_pos_label_1_illiq + rel_neg_label_1_illiq + rel_pos_label_0_illiq + rel_neg_label_0_illiq

xurated = model_run_final.copy()
xurated = xurated[["ar", "const", "dividends","market_cap","volume", "uncert", "illiq", "surprise_percent",
 "positive_label_1","negative_label_1","positive_label_0","negative_label_0","negative_label_0",
 "positive_label_1_uncert","negative_label_1_uncert","positive_label_0_uncert","negative_label_0_uncert",
 "positive_label_1_illiq","negative_label_1_illiq","positive_label_0_illiq","negative_label_0_illiq",
 "rel_pos_label_1","rel_neg_label_1","rel_pos_label_0","rel_neg_label_0",
 "rel_pos_label_1_uncert","rel_neg_label_1_uncert","rel_pos_label_0_uncert","rel_neg_label_0_uncert",
 "rel_pos_label_1_illiq","rel_neg_label_1_illiq","rel_pos_label_0_illiq","rel_neg_label_0_illiq"]]

xurated.describe().T.to_excel("stat_table.xlsx")

gr_df = model_run[1].iloc[:,0:40].copy()

gr_df.columns

import seaborn

import plotly.express as px

gr_df[['positive', 'negative']]

px.line(gr_df, x =gr_df.index, y = gr_df[['positive', 'negative']])

ML.df_stock

def prepare_news_df( curated= True, Organisation = True):
  News_df = ML.df_news.copy()
  News_df =News_df.reset_index()

  Date_col= pd.to_datetime(News_df['Date'], utc=True, errors='coerce')
  News_df['Date'] = Date_col.dt.date





  News_df['positive*Label_0'] = News_df['positive'] * News_df['LABEL_0']
  News_df['negative*Label_0'] = News_df['negative'] * News_df['LABEL_0']


  ##Relevance adjustment
  News_df['Rel_pos*Label_1'] = News_df['Rel_pos'] * News_df['LABEL_1']
  News_df['Rel_neg*Label_1'] = News_df['Rel_neg'] * News_df['LABEL_1']
  News_df['Rel_pos*Label_0'] = News_df['Rel_pos'] * News_df['LABEL_0']
  News_df['Rel_neg*Label_0'] = News_df['Rel_neg'] * News_df['LABEL_0']

  Original_News_df = News_df.copy()

  if Organisation == True:
    News_df = News_df[News_df['B-ORG'] == 1]

  if curated==True:
    sources_df = pd.read_excel("/content/Sources_class.xlsx", header=1).fillna(0)

    News_df = News_df.merge(sources_df, on="Source", how='left')
    News_df =News_df.drop("Source", axis=1)
    News_df =News_df.drop("count", axis=1)
    News_df = News_df[(News_df['Relevant'] >0) | (News_df['Traditional'] >0)]




  News_df= News_df.groupby(['Company', 'Date']).sum()

  return News_df, Original_News_df

def merge_stock_news(news_df, stock_df, only_rel =True, no_Events=False ):

  df_stock = pd.merge(stock_df, news_df.reset_index(), left_on=['symbol', 'Date'],
                       right_on=['Company', 'Date'], how="left")


  print(df_stock)
  df_stock = df_stock[df_stock['Date'] >= pd.to_datetime('2020-01-01').date() ]
  df_stock = df_stock[df_stock['Date'] <= pd.to_datetime('2023-09-01').date() ]


  ## Market uncertanty index

  Uncertainty_df = pd.DataFrame()
  for date in df_stock.Date.unique():
    Set = df_stock[df_stock.Date == date]
    Positive_sum = Set['positive'].sum()
    Negative_sum = Set['negative'].sum()
    try:

      Uncert = max(0, 1- Negative_sum / (Positive_sum + Negative_sum))
      ##print(Positive_sum, Negative_sum, Uncert)

    except:
      Uncert = 0
    df_stock.loc[df_stock.Date == date, 'Uncert']  = Uncert
    day_dict = {"Date": date, "Uncert": Uncert}
    Uncertainty_df = pd.concat([Uncertainty_df, pd.DataFrame([day_dict])], ignore_index=True)
    ##Uncertainty_df = Uncertainty_df.append(day_dict)

  print(Uncertainty_df)

  ## Event window

  try:
    df_stock["Event_only_index"] =np.nan
    df_stock.loc[(df_stock["Event"]==1) & (df_stock["Event_ID"].notna()), "Event_only_index"] = df_stock["Event_ID"]

  except:
    print('Error in EVENT_ID column')


  ## Amihud computation

  df_stock["Illiquidity"] = abs(df_stock["Returns"])/(df_stock["High"] * df_stock["High"]*df_stock["Volume"]/2)

  grouped = df_stock.drop("Date", axis=1).groupby('symbol')
  print(grouped)


  results = {'symbol':[], 'illiquidity':[]}
  for item, grp in grouped:
    print(item)
    ##print(grp.tail(2))

    ##subset_mean = grp.tail(2).sum()[0]
    subset_mean =grp["Illiquidity"].mean()
    print(subset_mean)
    results['symbol'].append(item)
    results['illiquidity'].append(subset_mean)

  print(results)
  res_df = pd.DataFrame(results)
  Quant = res_df["illiquidity"].quantile(0.75)
  res_df['Illiq'] = res_df['illiquidity'].ge(Quant).astype(int)
  df_stock = df_stock.merge(res_df, on='symbol', how='left')




  df_stock.reset_index(inplace=True)



  if no_Events == True:

    df_stock["Date"] = pd.to_datetime(df_stock["Date"])

    test = df_stock[["Date",  'Volume', 'Dividends', 'Stock Splits', 'Market_Cap', 'SP500_returns', 'SP500_returns_yesterday', 'Returns', 'AR', 'EPS Estimate', 'Reported EPS',
       'Surprise(%)', 'symbol', 'positive*Label_1',
       'negative*Label_1', 'positive*Label_0', 'negative*Label_0', 'Uncert',
       'Event_only_index', 'Rel_pos*Label_1', 'Rel_neg*Label_1',
       'Rel_pos*Label_0', 'Rel_neg*Label_0', "Illiq",'PL1_20', 'NL1_20', 'PL0_20',
       'NL0_20', 'RPL1_20', 'RNL1_20', 'RPL0_20', 'RNL0_20', 'PL1_40',
       'NL1_40', 'PL0_40', 'NL0_40', 'RPL1_40', 'RNL1_40', 'RPL0_40',
       'RNL0_40', 'PL1_60', 'NL1_60', 'PL0_60', 'NL0_60', 'RPL1_60', 'RNL1_60',
       'RPL0_60', 'RNL0_60', 'PL1_120', 'NL1_120', 'PL0_120', 'NL0_120',
       'RPL1_120', 'RNL1_120', 'RPL0_120', 'RNL0_120']]
    model_run = test.groupby(['symbol', 'Date']).sum()
    ##return model_run, df_stock

  else:

    test = df_stock[['Date', 'Volume', 'Dividends', 'Stock Splits', 'Market_Cap', 'SP500_returns', 'SP500_returns_yesterday', 'Returns', 'AR', 'EPS Estimate', 'Reported EPS',
        'Surprise(%)', 'symbol', 'Event', 'Event_ID', 'positive*Label_1',
        'negative*Label_1', 'positive*Label_0', 'negative*Label_0', 'Uncert',
        'Event_only_index', 'Rel_pos*Label_1', 'Rel_neg*Label_1',
        'Rel_pos*Label_0', 'Rel_neg*Label_0', "Illiq"]]

    test2 = test.drop(test[test['Event_only_index'] == 0.0].index)
    ##model_run = test2.groupby(['symbol', 'Event_only_index']).sum()

    ##text_cols = df.columns[1:3]
    data_cols = set(test2.columns) - set(["Date", 'symbol'])
    d1 = dict.fromkeys(data_cols, 'sum')
    d2 = dict.fromkeys(["Date"], 'first')
    d = {**d1, **d2}

    model_run = test2.groupby(['symbol', 'Event_only_index']).agg(d)
    print(model_run)

    ## Fill in uncertainty index
    model_run.drop("Uncert", axis=1, inplace=True)

    def get_uncertainty(df, df_uncert):

      date = df['Date']
      Index_value = df_uncert[df_uncert['Date'] == date]['Uncert'].values[0]
      print(date, Index_value)
      return Index_value

    model_run['Uncert']= model_run.apply(lambda x: get_uncertainty(x, Uncertainty_df), axis =1)
  ##model_run= model_run.merge(Uncertainty_df, how='left', on=['Date'])
  print(model_run)
  ##model_run.set_index(['symbol', 'Event_only_index'], inplace=True)




  ##functions for other columns
  def new_dummy(df, key_col, col_to_apply):
    for i in col_to_apply:
      name = i +"_" + key_col
      print(name)
      df[name] = df[key_col]* df[i]
    return df





  Col_list = ['positive*Label_1', 'negative*Label_1', 'positive*Label_0',
              'negative*Label_0', 'Rel_pos*Label_1', 'Rel_neg*Label_1',
              'Rel_pos*Label_0', 'Rel_neg*Label_0']
  model_run = new_dummy(model_run, "Uncert", Col_list)
  model_run = new_dummy(model_run, "Illiq", Col_list)




  return model_run, df_stock

News_df = prepare_news_df(curated= True, Organisation = True)

model_run = merge_stock_news(News_df[0], df_stock, only_rel=False, no_Events=False )

df_stock

grdf = model_run[1].groupby('Date').sum().reset_index()

grdf

grdf.drop('symbol', axis=1).to_excel("graphs.xlsx")

fig = px.line(grdf, x = 'Date', y = ['positive'])
fig

fig

import seaborn as sns

sns.lineplot(x="Date", y="positive",
             data=grdf)

